import os
import sys
import logging
import pprint

# --- ä¿®æ­£æ¨¡çµ„åŒ¯å…¥è·¯å¾‘ ---
# å–å¾—ç›®å‰è…³æœ¬çš„ç›®éŒ„ï¼Œç„¶å¾Œå¾€ä¸Šä¸€å±¤æ‰¾åˆ°å°ˆæ¡ˆæ ¹ç›®éŒ„
# å†å°‡ src ç›®éŒ„åŠ å…¥åˆ° Python çš„æœå°‹è·¯å¾‘ä¸­
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.join(os.path.dirname(SCRIPT_DIR), "src")
sys.path.insert(0, SRC_DIR)

from tools.drive_downloader import download_file
from tools.pdf_parser import parse_pdf
from tools.report_generator import setup_font, generate_final_report
from tools.gemini_manager import GeminiManager

def main_pipeline():
    """
    ä¸»åŸ·è¡Œæµç¨‹ï¼Œä¸²è¯æ‰€æœ‰æ¨¡çµ„å®Œæˆå¾ä¸‹è¼‰åˆ°ç”Ÿæˆå ±å‘Šçš„å®Œæ•´ä»»å‹™ã€‚
    """
    print("="*60 + "\nğŸš€ å•Ÿå‹•æ¨¡çµ„åŒ–è™•ç†æµç¨‹\n" + "="*60)

    # --- åƒæ•¸è¨­å®š ---
    API_KEY = os.environ.get("GOOGLE_API_KEY")
    if not API_KEY:
        logging.error("âŒ æµç¨‹çµ‚æ­¢ï¼šè«‹å…ˆè¨­å®š GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸ã€‚")
        return

    urls_to_process = {
        "lai_jie_6799": "https://drive.google.com/file/d/1RUl7XhxyJpxKO4RBX0AxeeyD4ABYPU_l/view?usp=sharing",
        "jing_que_3162": "https://docs.google.com/document/d/16TkL54YmFAToS1UR26VdV_mYgr8bCAml/edit?tab=t.0",
    }
    # ä½¿ç”¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¾†å®šç¾©è·¯å¾‘
    PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
    download_dir = os.path.join(PROJECT_ROOT, "downloads")
    image_dir = os.path.join(PROJECT_ROOT, "images")
    report_output_path = os.path.join(PROJECT_ROOT, "final_report.pdf")

    # --- æµç¨‹é–‹å§‹ ---
    print("\n[æ­¥é©Ÿ 1/5] æ­£åœ¨æª¢æŸ¥ä¸¦è¨­å®šä¸­æ–‡å­—é«”...")
    if not setup_font():
        print("\nâŒ æµç¨‹çµ‚æ­¢ï¼šä¸­æ–‡å­—é«”å®‰è£å¤±æ•—ã€‚")
        return

    print("\n[æ­¥é©Ÿ 2/5] æ­£åœ¨åˆå§‹åŒ– AI æ ¸å¿ƒç®¡ç†å™¨...")
    try:
        ai_manager = GeminiManager(api_keys=[{"name": "user_provided_key", "value": API_KEY}])
        print("âœ”ï¸ AI ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"\nâŒ æµç¨‹çµ‚æ­¢ï¼šAI ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        return

    all_results = []
    print("\n[æ­¥é©Ÿ 3/5] é–‹å§‹é€ä¸€è™•ç†æ–‡ä»¶...")
    for name, url in urls_to_process.items():
        print("\n" + "="*60 + f"\nâ–¶ï¸  é–‹å§‹è™•ç†æ–‡ä»¶: {name}\n" + "="*60)

        downloaded_pdf_path = download_file(url, download_dir, f"{name}.pdf")
        if not downloaded_pdf_path:
            continue

        extracted_content = parse_pdf(downloaded_pdf_path, image_dir)
        if not extracted_content:
            continue

        print("\n[æ­¥é©Ÿ 4/5] æ­£åœ¨é€²è¡Œ AI åˆ†æ...")
        text_analysis = ai_manager.analyze_text(extracted_content['text'])
        image_analyses = [ {img_path: ai_manager.describe_image(img_path)} for img_path in extracted_content['image_paths'] ]

        final_data = {
            "source_doc": name,
            "original_content": extracted_content,
            "ai_analysis": {"text_analysis": text_analysis, "image_analyses": image_analyses}
        }
        all_results.append(final_data)
        print("\n--- åˆ†æçµæœé è¦½ ---")
        pprint.pprint(final_data)
        print("--- é è¦½çµæŸ ---")

    print("\n[æ­¥é©Ÿ 5/5] æ­£åœ¨ç”Ÿæˆæœ€çµ‚çš„ PDF åˆ†æå ±å‘Š...")
    if all_results:
        # ç‚ºäº†ç¤ºç¯„ï¼Œæˆ‘å€‘åªç”¨ç¬¬ä¸€ä»½æ–‡ä»¶çš„çµæœä¾†ç”Ÿæˆå ±å‘Š
        generate_final_report(all_results[0], report_output_path)
    else:
        print("æ²’æœ‰ä»»ä½•æ–‡ä»¶æˆåŠŸè™•ç†ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚")

    print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶è™•ç†å®Œç•¢ï¼")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    main_pipeline()
