# -*- coding: utf-8 -*-
# tools/gemini_processor.py
import argparse
import json
import logging
import os
import re
import sys
import time
from pathlib import Path
import google.generativeai as genai
import concurrent.futures

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
log = logging.getLogger('gemini_processor_tool')

# --- è¼”åŠ©å‡½å¼ ---
def sanitize_filename(title: str, max_len: int = 60) -> str:
    if not title:
        title = "untitled_document"
    title = re.sub(r'[\\/*?:"<>|]', "_", title)
    title = title.replace(" ", "_")
    title = re.sub(r"_+", "_", title)
    title = title.strip('_')
    return title[:max_len]

def print_progress(status: str, detail: str, extra_data: dict = None):
    progress_data = {"type": "progress", "status": status, "detail": detail}
    if extra_data:
        progress_data.update(extra_data)
    print(json.dumps(progress_data), file=sys.stderr, flush=True)

# --- æç¤ºè©ç®¡ç† ---
PROMPTS_FILE_PATH = Path(__file__).resolve().parent.parent / "prompts" / "default_prompts.json"

def load_prompts() -> dict:
    try:
        with open(PROMPTS_FILE_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        log.critical(f"ğŸ”´ ç„¡æ³•è¼‰å…¥æˆ–è§£ææç¤ºè©æª”æ¡ˆ: {PROMPTS_FILE_PATH}ã€‚éŒ¯èª¤: {e}", exc_info=True)
        sys.exit(1)

ALL_PROMPTS = load_prompts()

# --- éŒ¯èª¤è™•ç† ---
GEMINI_ERROR_MAP = {
    "SAFETY": "è™•ç†å¤±æ•—ï¼šå…§å®¹å¯èƒ½æ¶‰åŠå®‰å…¨æˆ–æ•æ„Ÿè­°é¡Œã€‚",
    "RECITATION": "è™•ç†å¤±æ•—ï¼šå…§å®¹å¯èƒ½å¼•ç”¨äº†å—ç‰ˆæ¬Šä¿è­·çš„è³‡æ–™ã€‚",
    "OTHER": "è™•ç†å¤±æ•—ï¼šå› æœªçŸ¥çš„æ¨¡å‹å…§éƒ¨åŸå› çµ‚æ­¢ã€‚"
}

def get_error_message_from_response(response):
    try:
        if response.prompt_feedback.block_reason:
            reason = response.prompt_feedback.block_reason.name
            return GEMINI_ERROR_MAP.get(reason, f"è«‹æ±‚è¢«æœªçŸ¥åŸå› é˜»æ“‹: {reason}")
        candidate = response.candidates[0]
        if candidate.finish_reason.name not in ("STOP", "MAX_TOKENS"):
            reason = candidate.finish_reason.name
            return GEMINI_ERROR_MAP.get(reason, f"è™•ç†ç•°å¸¸çµ‚æ­¢: {reason}")
    except (AttributeError, IndexError):
        return None
    return None

# --- æ ¸å¿ƒ Gemini è™•ç†å‡½å¼ ---

def list_models():
    """åˆ—å‡ºå¯ç”¨çš„ Gemini æ¨¡å‹ä¸¦ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œå¸¶æœ‰å¼·åˆ¶è¶…æ™‚ã€‚"""
    def list_models_task():
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("API Key not found in environment variables.")
        genai.configure(api_key=api_key)
        models_list = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                 models_list.append({"id": m.name, "name": m.display_name})
        return models_list

    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        try:
            future = executor.submit(list_models_task)
            models_list = future.result(timeout=30)
            print(json.dumps(models_list), flush=True)
        except ValueError as e:
            log.critical(f"ğŸ”´ åˆ—å‡ºæ¨¡å‹å¤±æ•—: {e}")
            print(f"Error listing models: {e}", file=sys.stderr, flush=True)
            sys.exit(1)
        except concurrent.futures.TimeoutError:
            log.critical(f"ğŸ”´ åˆ—å‡ºæ¨¡å‹è¶…æ™‚ï¼æ“ä½œåœ¨ 30 ç§’å…§æœªèƒ½å®Œæˆã€‚")
            print("Error listing models: Timeout after 30 seconds.", file=sys.stderr, flush=True)
            sys.exit(1)
        except Exception as e:
            log.critical(f"ğŸ”´ Failed to list models: {e}", exc_info=True)
            print(f"Error listing models: {e}", file=sys.stderr, flush=True)
            sys.exit(1)

def validate_key():
    """åƒ…é©—è­‰ API é‡‘é‘°çš„æœ‰æ•ˆæ€§ã€‚"""
    try:
        import google.api_core.exceptions
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print("éŒ¯èª¤ï¼šæœªåœ¨ç’°å¢ƒè®Šæ•¸ä¸­æä¾› GOOGLE_API_KEYã€‚", file=sys.stderr, flush=True)
            sys.exit(1)

        genai.configure(api_key=api_key)
        # åŸ·è¡Œä¸€å€‹è¼•é‡ç´šçš„ API å‘¼å«ä¾†è§¸ç™¼é©—è­‰
        next(genai.list_models(), None)

        log.info("âœ… API é‡‘é‘°é©—è­‰æˆåŠŸã€‚")
        sys.exit(0)

    except google.api_core.exceptions.InvalidArgument as e:
        print(f"é‡‘é‘°é©—è­‰å¤±æ•—ï¼šç„¡æ•ˆçš„ API é‡‘é‘°æˆ–æ ¼å¼éŒ¯èª¤ã€‚Google API è¨Šæ¯: {e}", file=sys.stderr, flush=True)
        sys.exit(1)
    except google.api_core.exceptions.PermissionDenied as e:
        print(f"é‡‘é‘°é©—è­‰å¤±æ•—ï¼šæ¬Šé™è¢«æ‹’çµ•ã€‚è«‹æª¢æŸ¥æ‚¨çš„é‡‘é‘°æ˜¯å¦æœ‰æ¬Šé™å­˜å–è©²æœå‹™ã€‚Google API è¨Šæ¯: {e}", file=sys.stderr, flush=True)
        sys.exit(1)
    except Exception as e:
        print(f"é‡‘é‘°é©—è­‰æ™‚ç™¼ç”Ÿæœªé æœŸçš„ç¶²è·¯æˆ–å…¶ä»–éŒ¯èª¤ï¼š{e}", file=sys.stderr, flush=True)
        sys.exit(1)

def generate_content_with_timeout(model, prompt_parts: list, log_message: str, internal_timeout: int = 100):
    log.info(f"æ­£è¦å‘¼å« model.generate_content ({log_message})...")
    external_timeout = internal_timeout + 10
    def generation_task():
        try:
            return model.generate_content(prompt_parts, request_options={'timeout': internal_timeout})
        except Exception as e:
            log.error(f"generate_content åŸ·è¡Œç·’å…§éƒ¨ç™¼ç”ŸéŒ¯èª¤ ({log_message}): {e}", exc_info=True)
            raise
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        try:
            future = executor.submit(generation_task)
            response = future.result(timeout=external_timeout)
            log.info(f"model.generate_content ({log_message}) å‘¼å«æˆåŠŸè¿”å›ã€‚")
            return response
        except concurrent.futures.TimeoutError:
            log.critical(f"ğŸ”´ model.generate_content ({log_message}) è¶…æ™‚ï¼æ“ä½œåœ¨ {external_timeout} ç§’å…§æœªèƒ½å®Œæˆã€‚")
            raise RuntimeError(f"AI å…§å®¹ç”Ÿæˆæ“ä½œ '{log_message}' è¶…æ™‚ã€‚")
        except Exception as e:
            log.critical(f"ğŸ”´ model.generate_content ({log_message}) ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}", exc_info=True)
            raise

def upload_to_gemini(genai_module, audio_path: Path, display_filename: str):
    log.info(f"â˜ï¸ Uploading '{display_filename}' to Gemini Files API with a hard timeout...")
    print_progress("uploading", f"æ­£åœ¨ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ {display_filename}...")
    ext = audio_path.suffix.lower()
    mime_map = {'.mp3': 'audio/mp3', '.m4a': 'audio/m4a', '.aac': 'audio/aac', '.wav': 'audio/wav', '.ogg': 'audio/ogg', '.flac': 'audio/flac', '.webm': 'audio/webm', '.mp4': 'audio/mp4'}
    mime_type = mime_map.get(ext, 'application/octet-stream')
    if mime_type in ['audio/m4a', 'audio/mp4']:
        mime_type = 'audio/aac'
    def upload_task():
        log.info("æ­£è¦å‘¼å« genai.upload_file...")
        try:
            # ä¿®æ­£ï¼šç§»é™¤ä¸è¢«æ”¯æ´çš„ 'request_options' åƒæ•¸ã€‚
            # è¶…æ™‚æ§åˆ¶å®Œå…¨ç”±å¤–éƒ¨çš„ concurrent.futures.ThreadPoolExecutor çš„ future.result(timeout=...) ä¾†è™•ç†ã€‚
            return genai_module.upload_file(path=str(audio_path), display_name=display_filename, mime_type=mime_type)
        except Exception as e:
            log.error(f"æª”æ¡ˆä¸Šå‚³åŸ·è¡Œç·’å…§éƒ¨ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            raise
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        try:
            future = executor.submit(upload_task)
            audio_file_resource = future.result(timeout=110)
            log.info(f"âœ… Upload successful. Gemini File URI: {audio_file_resource.uri}")
            print_progress("upload_complete", "éŸ³è¨Šä¸Šå‚³æˆåŠŸã€‚")
            return audio_file_resource
        except concurrent.futures.TimeoutError:
            log.critical("ğŸ”´ æª”æ¡ˆä¸Šå‚³è¶…æ™‚ï¼æ“ä½œåœ¨ 110 ç§’å…§æœªèƒ½å®Œæˆã€‚")
            raise RuntimeError("æª”æ¡ˆä¸Šå‚³æ“ä½œè¶…æ™‚ï¼Œç¨‹åºè¢«å¼·åˆ¶çµ‚æ­¢ã€‚")
        except Exception as e:
            log.critical(f"ğŸ”´ Failed to upload file to Gemini: {e}", exc_info=True)
            raise

def get_summary_and_transcript(gemini_file_resource, model, video_title: str, original_filename: str):
    log.info(f"ğŸ¤– Requesting summary and transcript from model '{model.model_name}'...")
    print_progress("generating_transcript", "AI æ­£åœ¨ç”Ÿæˆæ‘˜è¦èˆ‡é€å­—ç¨¿...")
    prompt = ALL_PROMPTS['get_summary_and_transcript'].format(original_filename=original_filename, video_title=video_title)
    response = generate_content_with_timeout(model, [prompt, gemini_file_resource], "æ‘˜è¦èˆ‡é€å­—ç¨¿")
    full_response_text = response.text
    summary_match = re.search(r"\[é‡é»æ‘˜è¦é–‹å§‹\](.*?)\[é‡é»æ‘˜è¦çµæŸ\]", full_response_text, re.DOTALL)
    summary_text = summary_match.group(1).strip() if summary_match else "æœªæ“·å–åˆ°é‡é»æ‘˜è¦ã€‚"
    transcript_match = re.search(r"\[è©³ç´°é€å­—ç¨¿é–‹å§‹\](.*?)\[è©³ç´°é€å­—ç¨¿çµæŸ\]", full_response_text, re.DOTALL)
    transcript_text = transcript_match.group(1).strip() if transcript_match else "æœªæ“·å–åˆ°è©³ç´°é€å­—ç¨¿ã€‚"
    if "æœªæ“·å–åˆ°" in summary_text and "æœªæ“·å–åˆ°" in transcript_text and "---[é€å­—ç¨¿åˆ†éš”ç·š]---" not in full_response_text:
        transcript_text = full_response_text
        summary_text = "ï¼ˆè‡ªå‹•æ‘˜è¦å¤±æ•—ï¼Œè«‹åƒè€ƒä¸‹æ–¹é€å­—ç¨¿è‡ªè¡Œæ•´ç†ï¼‰"
    log.info("âœ… Successfully generated summary and transcript.")
    print_progress("transcript_generated", "æ‘˜è¦èˆ‡é€å­—ç¨¿ç”Ÿæˆå®Œç•¢ã€‚")
    return summary_text, transcript_text, response

def generate_html_report(summary_text: str, transcript_text: str, model, video_title: str):
    log.info(f"ğŸ¨ Requesting HTML report from model '{model.model_name}'...")
    print_progress("generating_html", "AI æ­£åœ¨ç¾åŒ–æ ¼å¼ä¸¦ç”Ÿæˆ HTML å ±å‘Š...")
    prompt = ALL_PROMPTS['format_as_html'].format(video_title_for_html=video_title, summary_text_for_html=summary_text, transcript_text_for_html=transcript_text)
    response = generate_content_with_timeout(model, [prompt], "HTMLå ±å‘Š")
    generated_html = response.text
    if generated_html.strip().startswith("```html"):
        generated_html = generated_html.strip()[7:]
    if generated_html.strip().endswith("```"):
        generated_html = generated_html.strip()[:-3]
    doctype_pos = generated_html.lower().find("<!doctype html>")
    if doctype_pos != -1:
        generated_html = generated_html[doctype_pos:]
    log.info("âœ… Successfully generated HTML report.")
    print_progress("html_generated", "HTML å ±å‘Šç”Ÿæˆå®Œç•¢ã€‚")
    return generated_html.strip(), response

def process_audio_file(audio_path: Path, model_name: str, video_title: str, output_dir: Path, tasks: str, output_format: str):
    start_time = time.time()
    total_tokens_used = 0
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY environment variable not set.")
    genai.configure(api_key=api_key)
    task_list = [t.strip() for t in tasks.lower().split(',') if t.strip()]
    results = {}
    gemini_file_resource = None
    try:
        gemini_file_resource = upload_to_gemini(genai, audio_path, audio_path.name)
        model_instance = genai.GenerativeModel(model_name)
        def get_token_count(response):
            try: return response.usage_metadata.total_token_count
            except: return 0
        if "summary" in task_list and "transcript" in task_list:
            summary, transcript, response = get_summary_and_transcript(gemini_file_resource, model_instance, video_title, audio_path.name)
            error_msg = get_error_message_from_response(response)
            if error_msg: raise ValueError(error_msg)
            total_tokens_used += get_token_count(response)
            results['summary'] = summary
            results['transcript'] = transcript
        else:
            if "summary" in task_list:
                prompt = ALL_PROMPTS['get_summary_only'].format(original_filename=audio_path.name, video_title=video_title)
                response = generate_content_with_timeout(model_instance, [prompt, gemini_file_resource], "åƒ…æ‘˜è¦")
                error_msg = get_error_message_from_response(response)
                if error_msg: raise ValueError(error_msg)
                total_tokens_used += get_token_count(response)
                results['summary'] = response.text.strip()
            if "transcript" in task_list:
                prompt = ALL_PROMPTS['get_transcript_only'].format(original_filename=audio_path.name, video_title=video_title)
                response = generate_content_with_timeout(model_instance, [prompt, gemini_file_resource], "åƒ…é€å­—ç¨¿")
                error_msg = get_error_message_from_response(response)
                if error_msg: raise ValueError(error_msg)
                total_tokens_used += get_token_count(response)
                results['transcript'] = response.text.strip()
        sanitized_title = sanitize_filename(video_title)
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        final_filename_base = f"{sanitized_title}_{timestamp}_AI_Report"
        html_report_path = None
        txt_report_path = None

        if output_format == 'html':
            html_content, response = generate_html_report(results.get('summary', ''), results.get('transcript', ''), model_instance, video_title)
            error_msg = get_error_message_from_response(response)
            if error_msg: raise ValueError(error_msg)
            total_tokens_used += get_token_count(response)
            output_path = output_dir / f"{final_filename_base}.html"
            with open(output_path, "w", encoding="utf-8") as f: f.write(html_content)
            html_report_path = str(output_path)
        else: # Handle 'txt' format
            summary_text = results.get('summary', 'ç„¡æ‘˜è¦ã€‚')
            transcript_text = results.get('transcript', 'ç„¡é€å­—ç¨¿ã€‚')
            full_text_content = f"# {video_title}\n\n## é‡é»æ‘˜è¦\n\n{summary_text}\n\n---\n\n## è©³ç´°é€å­—ç¨¿\n\n{transcript_text}"
            output_path = output_dir / f"{final_filename_base}.txt"
            with open(output_path, "w", encoding="utf-8") as f: f.write(full_text_content)
            txt_report_path = str(output_path)

        final_result = {
            "type": "result",
            "status": "å·²å®Œæˆ",
            "output_path": str(output_path),
            "video_title": video_title,
            "total_tokens_used": total_tokens_used,
            "processing_duration_seconds": round(time.time() - start_time, 2),
            "html_report_path": html_report_path,
            "txt_report_path": txt_report_path
        }
        print(json.dumps(final_result), flush=True)
    except Exception as e:
        log.critical(f"ğŸ”´ è™•ç†æµç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}", exc_info=True)
        raise
    finally:
        if gemini_file_resource:
            log.info(f"ğŸ—‘ï¸ Cleaning up Gemini file: {gemini_file_resource.name}")
            try:
                for attempt in range(3):
                    try:
                        genai.delete_file(gemini_file_resource.name)
                        log.info("âœ… Cleanup successful.")
                        break
                    except Exception as e_del:
                        log.warning(f"Attempt {attempt+1} to delete file failed: {e_del}")
                        if attempt < 2: time.sleep(2)
                        else: raise
            except Exception as e:
                log.error(f"ğŸ”´ Failed to clean up Gemini file '{gemini_file_resource.name}' after retries: {e}")

def main():
    parser = argparse.ArgumentParser(description="Gemini AI è™•ç†å·¥å…·ã€‚")
    parser.add_argument("--command", type=str, default="process", choices=["process", "list_models", "validate_key"], help="è¦åŸ·è¡Œçš„æ“ä½œã€‚")
    args, remaining_argv = parser.parse_known_args()
    if args.command == "list_models":
        list_models()
    elif args.command == "validate_key":
        validate_key()
    elif args.command == "process":
        process_parser = argparse.ArgumentParser()
        process_parser.add_argument("--command", type=str, help=argparse.SUPPRESS)
        process_parser.add_argument("--audio-file", type=str, required=True, help="è¦è™•ç†çš„éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ã€‚")
        process_parser.add_argument("--model", type=str, required=True, help="è¦ä½¿ç”¨çš„ Gemini æ¨¡å‹ API åç¨±ã€‚")
        process_parser.add_argument("--video-title", type=str, required=True, help="åŸå§‹å½±ç‰‡æ¨™é¡Œï¼Œç”¨æ–¼æç¤ºè©ã€‚")
        process_parser.add_argument("--output-dir", type=str, required=True, help="å„²å­˜ç”Ÿæˆå ±å‘Šçš„ç›®éŒ„ã€‚")
        process_parser.add_argument("--tasks", type=str, default="summary,transcript", help="è¦åŸ·è¡Œçš„ä»»å‹™åˆ—è¡¨ã€‚")
        process_parser.add_argument("--output-format", type=str, default="html", choices=["html", "txt"], help="æœ€çµ‚è¼¸å‡ºçš„æª”æ¡ˆæ ¼å¼ã€‚")
        process_args = process_parser.parse_args(remaining_argv)
        audio_path = Path(process_args.audio_file)
        if not audio_path.exists():
            log.critical(f"Input audio file not found: {audio_path}")
            print(json.dumps({"type": "result", "status": "failed", "error": f"Input file not found: {audio_path}"}), flush=True)
            sys.exit(1)
        try:
            process_audio_file(audio_path=audio_path, model_name=process_args.model, video_title=process_args.video_title, output_dir=Path(process_args.output_dir), tasks=process_args.tasks, output_format=process_args.output_format)
        except Exception as e:
            log.critical(f"An error occurred in the main processing flow: {e}", exc_info=True)
            print(json.dumps({"type": "result", "status": "failed", "error": str(e)}), flush=True)
            sys.exit(1)

if __name__ == "__main__":
    main()
