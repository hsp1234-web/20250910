# api_server.py
import uuid
import shutil
import logging
import json
import subprocess
import sys
import threading
import re
import asyncio
import os
import time
from fastapi import FastAPI, UploadFile, File, Form, Request, HTTPException, WebSocket, WebSocketDisconnect, Query
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from typing import Optional, Dict, List
from contextlib import asynccontextmanager
from urllib.parse import unquote, quote
from pydantic import BaseModel
import psutil

# --- ä¿®æ­£æ¨¡çµ„åŒ¯å…¥è·¯å¾‘ ---
# å°‡å°ˆæ¡ˆçš„ src ç›®éŒ„æ–°å¢åˆ° Python çš„æœå°‹è·¯å¾‘ä¸­ï¼Œ
# é€™æ¨£æ‰èƒ½æ­£ç¢ºæ‰¾åˆ° db.client ç­‰æ¨¡çµ„ã€‚
SRC_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(SRC_DIR))

from db.client import get_client

# --- JULES æ–¼ 2025-08-09 çš„ä¿®æ”¹ï¼šè¨­å®šæ‡‰ç”¨ç¨‹å¼å…¨åŸŸæ™‚å€ ---
# ç‚ºäº†ç¢ºä¿æ‰€æœ‰æ—¥èªŒå’Œè³‡æ–™åº«æ™‚é–“æˆ³éƒ½ä½¿ç”¨ä¸€è‡´çš„æ™‚å€ï¼Œæˆ‘å€‘åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•çš„
# æœ€æ—©æœŸéšæ®µå°±å°‡æ™‚å€ç’°å¢ƒè®Šæ•¸è¨­å®šç‚º 'Asia/Taipei'ã€‚
os.environ['TZ'] = 'Asia/Taipei'
if sys.platform != 'win32':
    time.tzset()
# --- æ™‚å€è¨­å®šçµæŸ ---

# --- æ¨¡å¼è¨­å®š ---
# JULES: æ”¹ç‚ºé€éç’°å¢ƒè®Šæ•¸ä¾†æ±ºå®šæ¨¡æ“¬æ¨¡å¼ï¼Œä»¥ä¾¿èˆ‡ Circus æ•´åˆ
# é è¨­ç‚ºéæ¨¡æ“¬æ¨¡å¼ (çœŸå¯¦æ¨¡å¼)
IS_MOCK_MODE = os.environ.get("API_MODE", "real") == "mock"

# --- è·¯å¾‘è¨­å®š ---
# ä»¥æ­¤æª”æ¡ˆç‚ºåŸºæº–ï¼Œå®šç¾©å°ˆæ¡ˆæ ¹ç›®éŒ„
# å› ç‚ºæ­¤æª”æ¡ˆç¾åœ¨ä½æ–¼ src/api/ ä¸­ï¼Œæ‰€ä»¥æ ¹ç›®éŒ„æ˜¯å…¶ä¸Šä¸Šå±¤ç›®éŒ„
ROOT_DIR = Path(__file__).resolve().parent.parent.parent

# --- ä¸»æ—¥èªŒè¨­å®š ---
# ä¸»æ—¥èªŒå™¨
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()] # è¼¸å‡ºåˆ°æ§åˆ¶å°
)
log = logging.getLogger('api_server')

def setup_database_logging():
    """è¨­å®šè³‡æ–™åº«æ—¥èªŒè™•ç†å™¨ã€‚"""
    try:
        from db.log_handler import DatabaseLogHandler
        root_logger = logging.getLogger()
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰åŒé¡å‹çš„ handlerï¼Œé¿å…é‡è¤‡åŠ å…¥
        if not any(isinstance(h, DatabaseLogHandler) for h in root_logger.handlers):
            root_logger.addHandler(DatabaseLogHandler(source='api_server'))
            log.info("è³‡æ–™åº«æ—¥èªŒè™•ç†å™¨è¨­å®šå®Œæˆ (source: api_server)ã€‚")
    except Exception as e:
        log.error(f"æ•´åˆè³‡æ–™åº«æ—¥èªŒæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


# Frontend action logging is now handled by the centralized database logger.


# --- WebSocket é€£ç·šç®¡ç†å™¨ ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        log.info(f"æ–°ç”¨æˆ¶ç«¯é€£ç·šã€‚ç›®å‰å…± {len(self.active_connections)} å€‹é€£ç·šã€‚")

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        log.info(f"ä¸€å€‹ç”¨æˆ¶ç«¯é›¢ç·šã€‚ç›®å‰å…± {len(self.active_connections)} å€‹é€£ç·šã€‚")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)

    async def broadcast_json(self, data: dict):
        for connection in self.active_connections:
            await connection.send_json(data)

manager = ConnectionManager()


# --- DB å®¢æˆ¶ç«¯ ---
# åœ¨æ¨¡çµ„åŠ è¼‰æ™‚ç²å–å®¢æˆ¶ç«¯å–®ä¾‹
# å®¢æˆ¶ç«¯å…§éƒ¨æœ‰é‡è©¦æ©Ÿåˆ¶ï¼Œæœƒç­‰å¾… DB ç®¡ç†è€…æœå‹™å°±ç·’
db_client = get_client()

# --- FastAPI Lifespan Manager ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œçš„ç¨‹å¼ç¢¼
    setup_database_logging()
    log.info("è³‡æ–™åº«æ—¥èªŒè™•ç†å™¨å·²é€é lifespan äº‹ä»¶è¨­å®šã€‚")
    yield
    # å¯ä»¥åœ¨æ­¤è™•åŠ å…¥æ‡‰ç”¨ç¨‹å¼é—œé–‰æ™‚åŸ·è¡Œçš„ç¨‹å¼ç¢¼

# --- FastAPI æ‡‰ç”¨å¯¦ä¾‹ ---
app = FastAPI(title="é³³å‡°éŸ³è¨Šè½‰éŒ„å„€ API (v3 - é‡æ§‹)", version="3.0", lifespan=lifespan)

# --- ä¸­ä»‹è»Ÿé«” (Middleware) ---
# JULES: æ–°å¢ CORS ä¸­ä»‹è»Ÿé«”ä»¥å…è¨±ä¾†è‡ªç€è¦½å™¨è…³æœ¬çš„è·¨ä¾†æºè«‹æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è¨±æ‰€æœ‰ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],  # å…è¨±æ‰€æœ‰æ–¹æ³•
    allow_headers=["*"],  # å…è¨±æ‰€æœ‰æ¨™é ­
)

# --- æ•´åˆæ¨¡çµ„åŒ–è·¯ç”± ---
from api.routes import ui, page1_ingestion, page2_downloader, page3_processor, page4_analyzer, page5_backup

# UI è·¯ç”± (æä¾› HTML é é¢)
app.include_router(ui.router, tags=["UI"])

# API è·¯ç”± (æä¾›è³‡æ–™ä»‹é¢)
app.include_router(page1_ingestion.router, prefix="/api/ingestion", tags=["API: ç¶²å€æå–"])
app.include_router(page2_downloader.router, prefix="/api/downloader", tags=["API: æ‰¹æ¬¡ä¸‹è¼‰"])
app.include_router(page3_processor.router, prefix="/api/processor", tags=["API: æª”æ¡ˆè™•ç†"])
app.include_router(page4_analyzer.router, prefix="/api/analyzer", tags=["API: AI åˆ†æèˆ‡æç¤ºè©"])
app.include_router(page5_backup.router, prefix="/api/backup", tags=["API: å‚™ä»½ç®¡ç†"])

# --- è·¯å¾‘è¨­å®š ---
# æ–°çš„ä¸Šå‚³æª”æ¡ˆå„²å­˜ç›®éŒ„
UPLOADS_DIR = ROOT_DIR / "uploads"
REPORTS_DIR = ROOT_DIR / "reports"
# éœæ…‹æª”æ¡ˆç›®éŒ„
STATIC_DIR = ROOT_DIR / "src" / "static"

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
UPLOADS_DIR.mkdir(exist_ok=True)
REPORTS_DIR.mkdir(exist_ok=True)
if not STATIC_DIR.exists():
    log.warning(f"éœæ…‹æª”æ¡ˆç›®éŒ„ {STATIC_DIR} ä¸å­˜åœ¨ï¼Œå‰ç«¯é é¢å¯èƒ½ç„¡æ³•è¼‰å…¥ã€‚")
else:
    app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")
    app.mount("/reports", StaticFiles(directory=REPORTS_DIR), name="reports")
    # JULES'S FIX (2025-08-13): ç§»é™¤æœ‰å•é¡Œçš„ StaticFiles æ›è¼‰ï¼Œæ”¹ç”¨è‡ªè¨‚ç«¯é»

# JULES'S FIX (2025-08-13): æ ¹æ“šè¨ˆç•«ï¼Œæ–°å¢æ­¤ç«¯é»ä¾†è™•ç†è¤‡é›œæª”å
@app.get("/media/{file_path:path}")
async def serve_media_files(file_path: str):
    """
    ä¸€å€‹æ–°çš„APIç«¯é»ï¼Œå°ˆé–€ç”¨ä¾†å®‰å…¨åœ°æä¾›åª’é«”æª”æ¡ˆã€‚
    å®ƒæœƒæ‰‹å‹•è™•ç†URLè§£ç¢¼ï¼Œä»¥è§£æ±ºè¤‡é›œæª”åçš„å•é¡Œã€‚
    """
    try:
        # URL è§£ç¢¼ï¼Œå°‡ %20 è½‰ç‚ºç©ºæ ¼ï¼Œè™•ç†ä¸­æ–‡ç­‰
        decoded_path = unquote(file_path)
        # å»ºç«‹ä¸€å€‹å®‰å…¨çš„è·¯å¾‘ï¼Œé¿å…è·¯å¾‘éæ­·æ”»æ“Š
        safe_path = os.path.normpath(os.path.join(UPLOADS_DIR, decoded_path))

        # å†æ¬¡ç¢ºèªè·¯å¾‘æ˜¯åœ¨ UPLOADS_DIR ä¸‹
        if not safe_path.startswith(str(UPLOADS_DIR)):
             raise HTTPException(status_code=403, detail="ç¦æ­¢å­˜å–ã€‚")

        if os.path.exists(safe_path) and os.path.isfile(safe_path):
            return FileResponse(safe_path)
        else:
            log.warning(f"è«‹æ±‚çš„åª’é«”æª”æ¡ˆä¸å­˜åœ¨: {safe_path}")
            return JSONResponse(status_code=404, content={"detail": "File not found"})
    except Exception as e:
        log.error(f"æœå‹™åª’é«”æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"detail": str(e)})


def convert_to_media_url(absolute_path_str: str) -> str:
    """å°‡çµ•å°æª”æ¡ˆç³»çµ±è·¯å¾‘è½‰æ›ç‚ºå¯å…¬é–‹å­˜å–çš„ /media URLã€‚"""
    try:
        absolute_path = Path(absolute_path_str)
        # å°‹æ‰¾ç›¸å°æ–¼ UPLOADS_DIR çš„è·¯å¾‘
        relative_path = absolute_path.relative_to(UPLOADS_DIR)
        # å°‡è·¯å¾‘çš„æ¯å€‹éƒ¨åˆ†éƒ½é€²è¡Œ URL ç·¨ç¢¼ï¼Œä»¥è™•ç†ç‰¹æ®Šå­—å…ƒ
        # safe='' åƒæ•¸ç¢ºä¿é€£ '/' ä¹Ÿæœƒè¢«ç·¨ç¢¼ï¼Œå¦‚æœæœ‰çš„è©±
        encoded_path = '/'.join(quote(part, safe='') for part in relative_path.parts)

        # JULES DEBUG (2025-08-31): æ ¹æ“šæœ€æ–°åˆ†æå ±å‘Šï¼Œæ­¤è™•æ˜¯é€ æˆåª’é«”é è¦½å¤±æ•—çš„é—œéµã€‚
        # èˆŠçš„å¯«æ³• `relative_path.as_posix()` æ²’æœ‰å°æª”åä¸­çš„ '#' æˆ–ç©ºæ ¼ç­‰ç‰¹æ®Šå­—å…ƒé€²è¡Œç·¨ç¢¼ï¼Œ
        # å°è‡´ç€è¦½å™¨ç„¡æ³•æ­£ç¢ºè«‹æ±‚ URLã€‚æ–°çš„å¯«æ³•ä½¿ç”¨ `urllib.parse.quote` é€²è¡Œäº†ä¿®æ­£ã€‚
        # æ³¨æ„ï¼šæˆ‘å€‘åªå°è·¯å¾‘çš„ã€Œéƒ¨åˆ†ã€é€²è¡Œç·¨ç¢¼ï¼Œè€Œä¸æ˜¯æ•´å€‹ URLï¼Œä»¥ä¿ç•™æ–œç·šåˆ†éš”ç¬¦ã€‚

        # ä½¿ç”¨ quote å–ä»£ as_posix() ä¾†ç¢ºä¿ URL å®‰å…¨
        return f"/media/{encoded_path}"
    except (ValueError, TypeError):
        log.warning(f"ç„¡æ³•å°‡è·¯å¾‘ {absolute_path_str} è½‰æ›ç‚ºåª’é«” URLã€‚å›å‚³åŸå§‹è·¯å¾‘ã€‚")
        return absolute_path_str


# --- API ç«¯é» ---

@app.get("/", response_class=HTMLResponse)
async def serve_main_page(request: Request):
    """æ ¹ç«¯é»ï¼Œæä¾›å°ˆæ¡ˆçš„ä¸­å¿ƒä¸»é  (main.html)ã€‚"""
    html_file_path = STATIC_DIR / "main.html"
    if not html_file_path.is_file():
        log.error(f"æ‰¾ä¸åˆ°ä¸»é æª”æ¡ˆ: {html_file_path}")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°ä¸»é æª”æ¡ˆ (main.html)")
    return HTMLResponse(content=html_file_path.read_text(encoding="utf-8"), status_code=200)


def check_model_exists(model_size: str) -> bool:
    """
    æª¢æŸ¥æŒ‡å®šçš„ Whisper æ¨¡å‹æ˜¯å¦å·²ç¶“è¢«ä¸‹è¼‰åˆ°æœ¬åœ°å¿«å–ã€‚
    """
    # JULES'S FIX: å¢åŠ ä¸€å€‹ç’°å¢ƒè®Šæ•¸ä¾†å¼·åˆ¶ä½¿ç”¨æ¨¡æ“¬è½‰éŒ„å™¨ï¼Œä»¥æ”¯æ´æ··åˆæ¨¡å¼æ¸¬è©¦
    force_mock = os.environ.get("FORCE_MOCK_TRANSCRIBER") == "true"
    tool_script_path = ROOT_DIR / "src" / "tools" / ("mock_transcriber.py" if IS_MOCK_MODE or force_mock else "transcriber.py")
    log.info(f"ä½¿ç”¨ '{tool_script_path}' æª¢æŸ¥æ¨¡å‹ '{model_size}' æ˜¯å¦å­˜åœ¨...")

    # æˆ‘å€‘é€éå‘¼å«ä¸€å€‹è¼•é‡ç´šçš„å·¥å…·è…³æœ¬ä¾†æª¢æŸ¥ã€‚
    check_command = [sys.executable, str(tool_script_path), "--command=check", f"--model_size={model_size}"]
    try:
        # åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹ï¼Œmock_transcriber.py æœƒæ°¸é å›å‚³ "exists"
        result = subprocess.run(check_command, capture_output=True, text=True, check=True)
        output = result.stdout.strip().lower()
        log.info(f"æ¨¡å‹ '{model_size}' æª¢æŸ¥çµæœ: {output}")
        # å¿…é ˆå®Œå…¨åŒ¹é… "exists"ï¼Œé¿å… "not_exists" è¢«éŒ¯èª¤åˆ¤æ–·ç‚º True
        return output == "exists"
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        log.error(f"æª¢æŸ¥æ¨¡å‹ '{model_size}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

@app.post("/api/transcribe", status_code=202)
async def create_transcription_task(
    file: UploadFile = File(...),
    model_size: str = Form("tiny"),
    language: Optional[str] = Form(None),
    beam_size: int = Form(5)
):
    """
    æ¥æ”¶éŸ³è¨Šæª”æ¡ˆï¼Œæ ¹æ“šæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œæ±ºå®šæ˜¯ç›´æ¥å»ºç«‹è½‰éŒ„ä»»å‹™ï¼Œ
    é‚„æ˜¯å…ˆå»ºç«‹ä¸€å€‹ä¸‹è¼‰ä»»å‹™å’Œä¸€å€‹ä¾è³´æ–¼å®ƒçš„è½‰éŒ„ä»»å‹™ã€‚
    """
    # 1. æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_is_present = check_model_exists(model_size)

    # 2. ä¿å­˜ä¸Šå‚³çš„æª”æ¡ˆ
    transcribe_task_id = str(uuid.uuid4())
    file_extension = Path(file.filename).suffix or ".wav"
    saved_file_path = UPLOADS_DIR / f"{transcribe_task_id}{file_extension}"
    try:
        with open(saved_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        log.info(f"æª”æ¡ˆå·²å„²å­˜è‡³: {saved_file_path}")
    except Exception as e:
        log.error(f"âŒ å„²å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆ: {e}")
    finally:
        await file.close()

    # 3. æ ¹æ“šæ¨¡å‹æ˜¯å¦å­˜åœ¨ä¾†å»ºç«‹ä»»å‹™
    transcription_payload = {
        "input_file": str(saved_file_path),
        "original_filename": file.filename, # JULES'S FIX: Store the original filename
        "output_dir": "transcripts",
        "model_size": model_size,
        "language": language,
        "beam_size": beam_size
    }

    if model_is_present:
        # æ¨¡å‹å·²å­˜åœ¨ï¼Œç›´æ¥å»ºç«‹è½‰éŒ„ä»»å‹™
        log.info(f"âœ… æ¨¡å‹ '{model_size}' å·²å­˜åœ¨ï¼Œç›´æ¥å»ºç«‹è½‰éŒ„ä»»å‹™: {transcribe_task_id}")
        db_client.add_task(transcribe_task_id, json.dumps(transcription_payload), task_type='transcribe')
        # JULES: ä¿®æ­£ API å›æ‡‰ï¼Œä½¿å…¶èˆ‡å‰ç«¯çš„é€šç”¨è™•ç†é‚è¼¯ä¸€è‡´ï¼Œè£œä¸Š type æ¬„ä½
        return {"task_id": transcribe_task_id, "type": "transcribe"}
    else:
        # æ¨¡å‹ä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸‹è¼‰ä»»å‹™å’Œä¾è³´çš„è½‰éŒ„ä»»å‹™
        download_task_id = str(uuid.uuid4())
        log.warning(f"âš ï¸ æ¨¡å‹ '{model_size}' ä¸å­˜åœ¨ã€‚å»ºç«‹ä¸‹è¼‰ä»»å‹™ '{download_task_id}' å’Œä¾è³´çš„è½‰éŒ„ä»»å‹™ '{transcribe_task_id}'")

        download_payload = {"model_size": model_size}
        db_client.add_task(download_task_id, json.dumps(download_payload), task_type='download')

        db_client.add_task(transcribe_task_id, json.dumps(transcription_payload), task_type='transcribe', depends_on=download_task_id)

        # æˆ‘å€‘å›å‚³è½‰éŒ„ä»»å‹™çš„ IDï¼Œè®“å‰ç«¯å¯ä»¥è¿½è¹¤æœ€çµ‚çµæœ
        return JSONResponse(content={"tasks": [
            {"task_id": download_task_id, "type": "download"},
            {"task_id": transcribe_task_id, "type": "transcribe"}
        ]})


@app.get("/api/status/{task_id}")
async def get_task_status_endpoint(task_id: str):
    """
    æ ¹æ“šä»»å‹™ IDï¼Œå¾è³‡æ–™åº«æŸ¥è©¢ä»»å‹™ç‹€æ…‹ã€‚
    """
    log.debug(f"ğŸ” æ­£åœ¨æŸ¥è©¢ä»»å‹™ç‹€æ…‹: {task_id}")
    status_info = db_client.get_task_status(task_id)

    if not status_info:
        log.warning(f"â“ æ‰¾ä¸åˆ°ä»»å‹™ ID: {task_id}")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ ID")

    # DBClient å›å‚³çš„å·²ç¶“æ˜¯ dictï¼Œç„¡éœ€è½‰æ›
    response_data = status_info

    # å˜—è©¦è§£æ JSON çµæœ
    if response_data.get("result"):
        try:
            response_data["result"] = json.loads(response_data["result"])
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯åˆæ³•çš„ JSONï¼Œå°±ä»¥åŸå§‹å­—ä¸²å½¢å¼å›å‚³
            log.warning(f"ä»»å‹™ {task_id} çš„çµæœä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚")
            pass

    log.info(f"âœ… å›å‚³ä»»å‹™ {task_id} çš„ç‹€æ…‹: {response_data['status']}")
    return JSONResponse(content=response_data)


@app.post("/api/log/action", status_code=200)
async def log_action_endpoint(payload: Dict):
    """
    æ¥æ”¶å‰ç«¯ç™¼é€çš„æ“ä½œæ—¥èªŒï¼Œä¸¦é€éè³‡æ–™åº«æ—¥èªŒè™•ç†å™¨è¨˜éŒ„ã€‚
    """
    action = payload.get("action", "unknown_action")
    # ç²å–ä¸€å€‹å°ˆé–€çš„ logger ä¾†æ¨™è­˜é€™äº›æ—¥èªŒçš„ä¾†æºç‚º 'frontend_action'
    # DatabaseLogHandler æœƒæ“·å–é€™å€‹æ—¥èªŒï¼Œä¸¦å°‡å…¶èˆ‡ logger åç¨±ä¸€èµ·å­˜å…¥è³‡æ–™åº«
    action_logger = logging.getLogger('frontend_action')
    action_logger.info(action)

    log.info(f"ğŸ“ å·²å°‡å‰ç«¯æ“ä½œè¨˜éŒ„åˆ°è³‡æ–™åº«: {action}") # åŒæ™‚åœ¨ä¸»æ§å°ä¹Ÿé¡¯ç¤ºæ—¥èªŒ
    return {"status": "logged"}


@app.get("/api/application_status")
async def get_application_status():
    """
    ç²å–æ ¸å¿ƒæ‡‰ç”¨çš„ç‹€æ…‹ï¼Œä¾‹å¦‚æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥ã€‚
    """
    # TODO: é€™éƒ¨åˆ†å°‡åœ¨å¾ŒçºŒèˆ‡ worker ç‹€æ…‹åŒæ­¥
    return {
        "model_loaded": False,
        "active_model": None,
        "message": "ç­‰å¾…ä½¿ç”¨è€…æ“ä½œ"
    }

@app.get("/api/system/readiness")
async def system_readiness_check():
    """
    æª¢æŸ¥æ ¸å¿ƒä¾è³´ï¼ˆå¦‚ yt-dlpï¼‰æ˜¯å¦å·²æº–å‚™å°±ç·’ã€‚
    """
    # ä½¿ç”¨ shutil.which æª¢æŸ¥ yt-dlp æ˜¯å¦åœ¨ç³»çµ± PATH ä¸­ä¸”å¯åŸ·è¡Œ
    yt_dlp_path = shutil.which("yt-dlp")
    is_ready = yt_dlp_path is not None

    if is_ready:
        log.info(f"âœ… ç³»çµ±å°±ç·’æª¢æŸ¥ï¼šæˆåŠŸæ‰¾åˆ° yt-dlp æ–¼ {yt_dlp_path}")
        return {"ready": True}
    else:
        log.warning("âš ï¸ ç³»çµ±å°±ç·’æª¢æŸ¥ï¼šæ‰¾ä¸åˆ° yt-dlpã€‚å‰ç«¯åŠŸèƒ½å¯èƒ½å—é™ã€‚")
        return {"ready": False}

@app.get("/api/system_stats")
async def get_system_stats():
    """
    ç²å–ä¸¦å›å‚³ç•¶å‰çš„ç³»çµ±è³‡æºä½¿ç”¨ç‹€æ…‹ï¼ˆCPU, RAM, GPUï¼‰ã€‚
    """
    # CPU
    cpu_usage = psutil.cpu_percent(interval=0.1)

    # RAM
    ram = psutil.virtual_memory()
    ram_usage = ram.percent

    # GPU (é€é nvidia-smi)
    gpu_usage = None
    gpu_detected = False
    try:
        # åŸ·è¡Œ nvidia-smi å‘½ä»¤
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, check=True
        )
        # è§£æè¼¸å‡º
        gpu_usage = float(result.stdout.strip())
        gpu_detected = True
    except (FileNotFoundError, subprocess.CalledProcessError) as e:
        # nvidia-smi ä¸å­˜åœ¨æˆ–åŸ·è¡Œå¤±æ•—
        log.debug(f"ç„¡æ³•ç²å– GPU è³‡è¨Š: {e}")
        gpu_usage = None
        gpu_detected = False

    return {
        "cpu_usage": cpu_usage,
        "ram_usage": ram_usage,
        "gpu_usage": gpu_usage,
        "gpu_detected": gpu_detected,
    }


@app.get("/api/tasks")
async def get_all_tasks_endpoint():
    """
    ç²å–æ‰€æœ‰ä»»å‹™çš„åˆ—è¡¨ï¼Œç”¨æ–¼å‰ç«¯å±•ç¤ºã€‚
    """
    tasks = db_client.get_all_tasks()
    # å˜—è©¦è§£æ payload å’Œ result ä¸­çš„ JSON å­—ä¸²
    for task in tasks:
        try:
            if task.get("payload"):
                task["payload"] = json.loads(task["payload"])
        except (json.JSONDecodeError, TypeError):
            log.warning(f"ä»»å‹™ {task.get('task_id')} çš„ payload ä¸æ˜¯æœ‰æ•ˆçš„ JSONã€‚")
            pass # ä¿æŒåŸæ¨£
        try:
            if task.get("result"):
                task["result"] = json.loads(task["result"])
        except (json.JSONDecodeError, TypeError):
            log.warning(f"ä»»å‹™ {task.get('task_id')} çš„ result ä¸æ˜¯æœ‰æ•ˆçš„ JSONã€‚")
            pass # ä¿æŒåŸæ¨£
    return JSONResponse(content=tasks)


@app.get("/api/logs")
async def get_system_logs_endpoint(
    levels: List[str] = Query(None, alias="level"),
    sources: List[str] = Query(None, alias="source")
):
    """
    ç²å–ç³»çµ±æ—¥èªŒï¼Œå¯æŒ‰ç­‰ç´šå’Œä¾†æºé€²è¡Œç¯©é¸ã€‚
    """
    log.info(f"API: æ­£åœ¨æŸ¥è©¢ç³»çµ±æ—¥èªŒ (Levels: {levels}, Sources: {sources})")
    try:
        logs = db_client.get_system_logs(levels=levels, sources=sources)
        return JSONResponse(content=logs)
    except Exception as e:
        log.error(f"âŒ æŸ¥è©¢ç³»çµ±æ—¥èªŒæ™‚ API å‡ºéŒ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æŸ¥è©¢ç³»çµ±æ—¥èªŒæ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤")


@app.get("/api/download/{task_id}")
async def download_transcript(task_id: str):
    """
    æ ¹æ“šä»»å‹™ ID ä¸‹è¼‰è½‰éŒ„çµæœæª”æ¡ˆã€‚
    """
    task = db_client.get_task_status(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ IDã€‚")

    if task['status'] != 'å·²å®Œæˆ':
        raise HTTPException(status_code=400, detail="ä»»å‹™å°šæœªå®Œæˆï¼Œç„¡æ³•ä¸‹è¼‰ã€‚")

    try:
        # å¾ result æ¬„ä½è§£æå‡ºæª”å
        result_data = json.loads(task['result'])
        # ä¾åºæª¢æŸ¥å¯èƒ½çš„è·¯å¾‘éµåï¼Œä»¥æ”¯æ´æ‰€æœ‰ä»»å‹™é¡å‹
        output_filename = (
            result_data.get("transcript_path") or
            result_data.get("output_path") or
            result_data.get("html_report_path") or
            result_data.get("pdf_report_path")
        )

        if not output_filename:
            raise HTTPException(status_code=500, detail="ä»»å‹™çµæœä¸­æœªåŒ…å«æœ‰æ•ˆçš„æª”æ¡ˆè·¯å¾‘ã€‚")

        # JULES'S FIX 2025-08-14: å°‡ URL è·¯å¾‘è½‰æ›å›æª”æ¡ˆç³»çµ±çµ•å°è·¯å¾‘
        # è³‡æ–™åº«ä¸­å„²å­˜çš„æ˜¯åƒ /media/reports/report.html é€™æ¨£çš„ URLï¼Œ
        # æˆ‘å€‘éœ€è¦å°‡å…¶è½‰æ›å›åƒ /app/uploads/reports/report.html é€™æ¨£çš„çµ•å°æª”æ¡ˆç³»çµ±è·¯å¾‘ã€‚
        if output_filename.startswith('/media/'):
            # ç§»é™¤ '/media/' å‰ç¶´ä¸¦èˆ‡ä¸Šå‚³ç›®éŒ„åˆä½µ
            relative_path = output_filename.lstrip('/media/')
            # JULES'S FIX 2025-08-31: æ–°å¢ URL è§£ç¢¼æ­¥é©Ÿ
            # é€™æ˜¯è§£æ±ºã€Œæª”æ¡ˆåç¨±éé•·ã€éŒ¯èª¤çš„é—œéµã€‚å¾è³‡æ–™åº«å–å‡ºçš„è·¯å¾‘æ˜¯
            # URL ç·¨ç¢¼éçš„ (ä¾‹å¦‚ 'file%20name.txt')ï¼Œæˆ‘å€‘å¿…é ˆå°‡å…¶è§£ç¢¼å›
            # 'file name.txt' æ‰èƒ½è®“æª”æ¡ˆç³»çµ±æ‰¾åˆ°å®ƒã€‚
            decoded_relative_path = unquote(relative_path)
            file_path = UPLOADS_DIR / decoded_relative_path
        else:
            # ä½œç‚ºå‚™ç”¨ï¼Œå¦‚æœè·¯å¾‘ä¸æ˜¯ /media/ é–‹é ­ï¼Œå‰‡å‡è¨­å®ƒæ˜¯ä¸€å€‹çµ•å°è·¯å¾‘
            # é€™å¯ä»¥ä¿æŒå°èˆŠè³‡æ–™æ ¼å¼çš„ç›¸å®¹æ€§
            file_path = Path(output_filename)

        if not file_path.is_file():
            log.error(f"âŒ æª”æ¡ˆç³»çµ±ä¸­çš„æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            raise HTTPException(status_code=404, detail="æª”æ¡ˆéºå¤±æˆ–ç„¡æ³•è®€å–ã€‚")

        # æä¾›æª”æ¡ˆä¸‹è¼‰
        ext = file_path.suffix.lower()
        if ext == '.pdf':
            media_type = 'application/pdf'
        elif ext == '.html':
            media_type = 'text/html'
        elif ext == '.mp4':
            media_type = 'video/mp4'
        elif ext in ['.mp3', '.m4a', '.wav', '.flac']:
            media_type = f'audio/{ext.strip(".")}'
        else:
            media_type = 'text/plain'
        return FileResponse(path=file_path, filename=file_path.name, media_type=media_type)

    except (json.JSONDecodeError, KeyError) as e:
        log.error(f"âŒ è§£æä»»å‹™ {task_id} çš„çµæœæ™‚å‡ºéŒ¯: {e}")
        raise HTTPException(status_code=500, detail="ç„¡æ³•è§£æä»»å‹™çµæœã€‚")


@app.post("/api/rename/{task_id}", status_code=200)
async def rename_task_file(task_id: str, request: Request):
    """
    é‡æ–°å‘½åèˆ‡å·²å®Œæˆä»»å‹™é—œè¯çš„æª”æ¡ˆã€‚
    """
    log.info(f"æ”¶åˆ°é‡æ–°å‘½åä»»å‹™ {task_id} çš„è«‹æ±‚ã€‚")
    try:
        data = await request.json()
        new_filename_base = data.get("new_filename")
        if not new_filename_base:
            raise HTTPException(status_code=400, detail="è«‹æ±‚ä¸­æœªæä¾› 'new_filename'ã€‚")

        task = db_client.get_task_status(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ IDã€‚")
        if task['status'] != 'å·²å®Œæˆ':
            raise HTTPException(status_code=400, detail="åªèƒ½é‡æ–°å‘½åå·²å®Œæˆçš„ä»»å‹™ã€‚")

        result_data = json.loads(task['result'])
        old_path_str = result_data.get("output_path")
        if not old_path_str:
            raise HTTPException(status_code=500, detail="ä»»å‹™çµæœä¸­æ‰¾ä¸åˆ°æª”æ¡ˆè·¯å¾‘ã€‚")

        # JULES DEBUG (2025-08-31): æ ¹æ“šæœ€æ–°åˆ†æå ±å‘Šï¼Œæ­¤è™•æ˜¯é€ æˆé‡æ–°å‘½åå¤±æ•—çš„é—œéµã€‚
        # old_path_str æ˜¯ä¸€å€‹ URL è·¯å¾‘ (ä¾‹å¦‚ /media/file.mp4)ï¼Œè€Œä¸æ˜¯æª”æ¡ˆç³»çµ±è·¯å¾‘ã€‚
        # æˆ‘å€‘éœ€è¦å°‡å…¶è½‰æ›å›çµ•å°æª”æ¡ˆç³»çµ±è·¯å¾‘ (ä¾‹å¦‚ /app/uploads/file.mp4)ã€‚
        if old_path_str.startswith('/media/'):
            # ç§»é™¤ '/media/' å‰ç¶´ä¸¦èˆ‡ä¸Šå‚³ç›®éŒ„åˆä½µ
            relative_path = old_path_str.lstrip('/media/')
            # é€™è£¡éœ€è¦å° relative_path é€²è¡Œ URL è§£ç¢¼ï¼Œä»¥è™•ç†æª”åä¸­çš„ %20 ç­‰å­—å…ƒ
            decoded_relative_path = unquote(relative_path)
            old_path = UPLOADS_DIR / decoded_relative_path
        else:
            # ä½œç‚ºå‚™ç”¨ï¼Œå¦‚æœè·¯å¾‘ä¸æ˜¯ /media/ é–‹é ­ï¼Œå‰‡å‡è¨­å®ƒæ˜¯ä¸€å€‹çµ•å°è·¯å¾‘
            old_path = Path(old_path_str)

        file_extension = old_path.suffix
        new_path = old_path.with_name(f"{new_filename_base}{file_extension}")

        if old_path == new_path:
            return {"status": "success", "message": "æ–°èˆŠæª”åç›¸åŒï¼Œç„¡éœ€è®Šæ›´ã€‚", "new_filename": new_filename_base}

        if new_path.exists():
            raise HTTPException(status_code=409, detail=f"ç›®æ¨™æª”å {new_path.name} å·²å­˜åœ¨ã€‚")

        os.rename(old_path, new_path)
        log.info(f"æª”æ¡ˆå·²å¾ {old_path} é‡æ–°å‘½åç‚º {new_path}")

        # æ›´æ–°è³‡æ–™åº«ä¸­çš„çµæœ
        # JULES DEBUG (2025-08-31): é‡æ–°å‘½åå¾Œï¼Œæˆ‘å€‘éœ€è¦å°‡æ–°çš„ã€Œæª”æ¡ˆç³»çµ±è·¯å¾‘ã€è½‰æ›å›ã€Œåª’é«” URLã€ï¼Œ
        # ç„¶å¾Œå†å­˜å…¥è³‡æ–™åº«ï¼Œä»¥ä¿æŒè³‡æ–™æ ¼å¼çš„ä¸€è‡´æ€§ã€‚
        result_data["output_path"] = convert_to_media_url(str(new_path))
        result_data["video_title"] = new_filename_base

        db_client.update_task_status(task_id, 'å·²å®Œæˆ', json.dumps(result_data))
        log.info(f"å·²æ›´æ–°è³‡æ–™åº«ä¸­ä»»å‹™ {task_id} çš„çµæœã€‚")

        return {"status": "success", "message": "æª”æ¡ˆé‡æ–°å‘½åæˆåŠŸã€‚", "new_filename": new_filename_base}

    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="ç„¡æ³•è§£æä»»å‹™çµæœã€‚")
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°è¦é‡æ–°å‘½åçš„åŸå§‹æª”æ¡ˆã€‚")
    except Exception as e:
        log.error(f"âŒ é‡æ–°å‘½åæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}")




@app.post("/api/upload_cookies", status_code=200)
async def upload_cookies_file(file: UploadFile = File(...)):
    """
    æ¥æ”¶ä½¿ç”¨è€…ä¸Šå‚³çš„ cookies.txt æª”æ¡ˆä¸¦å„²å­˜ã€‚
    """
    if "cookies.txt" not in file.filename.lower():
        raise HTTPException(status_code=400, detail="æª”æ¡ˆåç¨±å¿…é ˆæ˜¯ 'cookies.txt' æˆ–åŒ…å«è©²å­—æ¨£ã€‚")

    cookies_path = UPLOADS_DIR / "cookies.txt"
    try:
        with open(cookies_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        log.info(f"ğŸª Cookies æª”æ¡ˆå·²å„²å­˜è‡³: {cookies_path}")
        return {"status": "success", "message": "Cookies æª”æ¡ˆä¸Šå‚³æˆåŠŸã€‚"}
    except Exception as e:
        log.error(f"âŒ å„²å­˜ Cookies æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å„²å­˜ Cookies æª”æ¡ˆ: {e}")
    finally:
        await file.close()


# --- YouTube åŠŸèƒ½ç›¸é—œ API ---

@app.post("/api/youtube/validate_api_key")
async def validate_api_key(request: Request):
    """æ¥æ”¶å‰ç«¯å‚³ä¾†çš„ API Key ä¸¦é€²è¡Œé©—è­‰ã€‚"""
    try:
        payload = await request.json()
        api_key = payload.get("api_key")
        if not api_key:
            raise HTTPException(status_code=400, detail="æœªæä¾› API é‡‘é‘°ã€‚")

        if IS_MOCK_MODE:
            log.info("æ¨¡æ“¬æ¨¡å¼ï¼šå°‡éç©º API é‡‘é‘°è¦–ç‚ºæœ‰æ•ˆã€‚")
            return {"valid": True}

        tool_script_path = ROOT_DIR / "src" / "tools" / "gemini_processor.py"
        cmd = [sys.executable, str(tool_script_path), "--command=validate_key"]

        # JULES'S FIX V3: å»ºç«‹ä¸€å€‹æœ€å°åŒ–çš„ä¹¾æ·¨ç’°å¢ƒä¾†åŸ·è¡Œé©—è­‰ã€‚
        # é€™æ˜¯ç‚ºäº†é˜²æ­¢ Google çš„å‡½å¼åº«è‡ªå‹•å¾æ²™ç®±ç’°å¢ƒä¸­ç¹¼æ‰¿ä»»ä½•ã€Œæ‡‰ç”¨ç¨‹å¼é è¨­æ†‘è­‰ã€ï¼Œ
        # å¾è€Œç¢ºä¿é©—è­‰éç¨‹åªä½¿ç”¨ä½¿ç”¨è€…æä¾›çš„ API é‡‘é‘°ã€‚
        minimal_env = {
            "PATH": os.environ.get("PATH", ""),
            "GOOGLE_API_KEY": api_key,
            # åœ¨æŸäº›ç³»çµ±ä¸Šï¼Œç‰¹åˆ¥æ˜¯ Windowsï¼Œéœ€è¦ SYSTEMROOTã€‚ç‚ºä¿éšªèµ·è¦‹åŠ å…¥ã€‚
            "SYSTEMROOT": os.environ.get("SYSTEMROOT", "")
        }

        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', env=minimal_env, check=False)

        if result.returncode == 0:
            log.info(f"API é‡‘é‘°é©—è­‰æˆåŠŸã€‚")
            return {"valid": True}
        else:
            log.warning(f"API é‡‘é‘°é©—è­‰å¤±æ•—ã€‚Stderr: {result.stderr.strip()}")
            error_message = result.stderr.strip()
            detail = error_message if error_message else "é‡‘é‘°é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸»æ§å°æ—¥èªŒä»¥äº†è§£è©³æƒ…ã€‚"
            return JSONResponse(status_code=400, content={"valid": False, "detail": detail})

    except Exception as e:
        log.error(f"é©—è­‰ API é‡‘é‘°æ™‚ç™¼ç”Ÿä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}")


class ApiKeyPayload(BaseModel):
    api_key: str

@app.post("/api/youtube/models")
async def get_youtube_models(payload: ApiKeyPayload):
    """
    ç²å–å¯ç”¨çš„ Gemini æ¨¡å‹åˆ—è¡¨ã€‚
    ç¾åœ¨æ¥æ”¶ä¸€å€‹åŒ…å« API é‡‘é‘°çš„ POST è«‹æ±‚ã€‚
    """
    # åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹ï¼Œå›å‚³ä¸€å€‹å›ºå®šçš„å‡åˆ—è¡¨
    if IS_MOCK_MODE:
        return {
            "models": [
                {"id": "gemini-pro-mock", "name": "Gemini Pro (æ¨¡æ“¬)"},
                {"id": "gemini-1.5-flash-mock", "name": "Gemini 1.5 Flash (æ¨¡æ“¬)"}
            ]
        }

    # çœŸå¯¦æ¨¡å¼ä¸‹ï¼Œå¾ gemini_processor.py ç²å–
    try:
        if not payload.api_key:
            raise HTTPException(status_code=400, detail="è«‹æ±‚ä¸­æœªæä¾› API é‡‘é‘°ã€‚")

        log.info(f"æ”¶åˆ°ä¾†è‡ªå‰ç«¯çš„ API é‡‘é‘°ï¼Œå°‡å…¶ç”¨æ–¼ç²å–æ¨¡å‹åˆ—è¡¨ã€‚")

        tool_script_path = ROOT_DIR / "src" / "tools" / "gemini_processor.py"
        cmd = [sys.executable, str(tool_script_path), "--command=list_models"]

        # JULES DEBUG (2025-08-31): æ ¹æ“šæœ€æ–°åˆ†æå ±å‘Šï¼Œæ­¤è™•æ˜¯ä¿®å¾©æ¨¡å‹è¼‰å…¥å¤±æ•—çš„é—œéµã€‚
        # èˆŠçš„é‚è¼¯å¯èƒ½ä¾è³´äº†ä¸ç©©å®šçš„ã€è·¨è«‹æ±‚çš„ç’°å¢ƒè®Šæ•¸ã€‚
        # æ–°çš„é‚è¼¯æ˜ç¢ºåœ°å°‡å¾ POST request body ä¸­æ”¶åˆ°çš„ api_key è¨­å®šåˆ°å­ç¨‹åºçš„ç’°å¢ƒè®Šæ•¸ä¸­ï¼Œ
        # ç¢ºä¿äº†æ¯æ¬¡å‘¼å«éƒ½ä½¿ç”¨æ­£ç¢ºçš„æ†‘è­‰ã€‚
        env = os.environ.copy()
        env["GOOGLE_API_KEY"] = payload.api_key

        result = subprocess.run(cmd, capture_output=True, text=True, check=True, encoding='utf-8', env=env)
        models = json.loads(result.stdout)
        return {"models": models}
    except subprocess.CalledProcessError as e:
        stderr_log = e.stderr.strip()
        log.error(f"ç²å– Gemini æ¨¡å‹åˆ—è¡¨å¤±æ•—ï¼Œå¯èƒ½æ˜¯å› ç‚º API é‡‘é‘°ç„¡æ•ˆã€‚Stderr: {stderr_log}")
        # å°‡æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯å‚³å›çµ¦å‰ç«¯
        if "API Key not found" in stderr_log:
            detail_message = "API é‡‘é‘°éºå¤±ã€‚è«‹ç¢ºèªå¾Œç«¯å·²æ­£ç¢ºæ¥æ”¶é‡‘é‘°ã€‚"
        elif "API key not valid" in stderr_log:
            detail_message = "API é‡‘é‘°ç„¡æ•ˆã€‚è«‹æª¢æŸ¥æ‚¨çš„é‡‘é‘°ã€‚"
        else:
            detail_message = "ç„¡æ³•ä½¿ç”¨æä¾›çš„ API é‡‘é‘°ç²å–æ¨¡å‹åˆ—è¡¨ï¼Œè«‹æª¢æŸ¥é‡‘é‘°æ¬Šé™æˆ–ç¶²è·¯é€£ç·šã€‚"
        raise HTTPException(status_code=401, detail=detail_message)
    except Exception as e:
        log.error(f"ç²å– Gemini æ¨¡å‹åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ç„¡æ³•ç²å– Gemini æ¨¡å‹åˆ—è¡¨ã€‚")


@app.post("/api/youtube/process", status_code=202)
async def process_youtube_urls(request: Request):
    """
    æ¥æ”¶ YouTube URLï¼Œä¸¦æ ¹æ“šå‰ç«¯å‚³ä¾†çš„åƒæ•¸ï¼Œå»ºç«‹å°æ‡‰çš„ä¸‹è¼‰å’Œ AI åˆ†æä»»å‹™ã€‚
    """
    payload = await request.json()
    requests_list = payload.get("requests", [])

    # JULES'S FIX: ç‚ºäº†ç›¸å®¹èˆŠçš„ local_run.py æ¸¬è©¦è…³æœ¬
    if not requests_list and "urls" in payload:
        log.warning("åµæ¸¬åˆ°èˆŠç‰ˆçš„ 'urls' è² è¼‰æ ¼å¼ï¼Œæ­£åœ¨é€²è¡Œç›¸å®¹è™•ç†ã€‚")
        requests_list = [{"url": url, "filename": None} for url in payload.get("urls", [])]


    # æ–°çš„å½ˆæ€§åƒæ•¸
    model = payload.get("model")
    tasks_to_run = payload.get("tasks", "summary,transcript") # e.g., "summary,transcript,translate"
    output_format = payload.get("output_format", "html") # "html" or "txt"
    download_only = payload.get("download_only", False)
    download_type = payload.get("download_type", "audio") # JULES'S NEW FEATURE
    api_key = payload.get("api_key") # å¯¦ç¾ç„¡ç‹€æ…‹ï¼Œå¾è«‹æ±‚ä¸­ç›´æ¥ç²å–é‡‘é‘°

    if not requests_list:
        # åœ¨åŠ å…¥ç›¸å®¹æ€§é‚è¼¯å¾Œï¼Œæ›´æ–°éŒ¯èª¤è¨Šæ¯
        raise HTTPException(status_code=400, detail="è«‹æ±‚ä¸­å¿…é ˆåŒ…å« 'requests' æˆ– 'urls'ã€‚")
    if not download_only and not model:
        raise HTTPException(status_code=400, detail="åŸ·è¡Œ AI åˆ†ææ™‚å¿…é ˆæä¾› 'model'ã€‚")
    if not download_only and not api_key:
        raise HTTPException(status_code=401, detail="åŸ·è¡Œ AI åˆ†ææ™‚å¿…é ˆæä¾› 'api_key'ã€‚")


    tasks = []
    for req_item in requests_list:
        url = req_item.get("url")
        filename = req_item.get("filename")

        if not url or not url.strip():
            continue

        task_id = str(uuid.uuid4())

        if download_only:
            # JULES'S NEW FEATURE: Pass download_type to payload
            task_payload = {"url": url, "output_dir": str(UPLOADS_DIR), "custom_filename": filename, "download_type": download_type}
            db_client.add_task(task_id, json.dumps(task_payload), task_type='youtube_download_only')
            tasks.append({"url": url, "task_id": task_id})
        else:
            download_task_id = task_id
            process_task_id = str(uuid.uuid4())

            # JULES'S NEW FEATURE: Pass download_type to download payload
            download_payload = {"url": url, "output_dir": str(UPLOADS_DIR), "custom_filename": filename, "download_type": download_type}
            # å°‡æ‰€æœ‰æ–°åƒæ•¸å­˜å…¥ process ä»»å‹™çš„ payload
            process_payload = {
                "model": model,
                "output_dir": "transcripts",
                "tasks": tasks_to_run,
                "output_format": output_format,
                "api_key": api_key # å°‡é‡‘é‘°å­˜å…¥ä»»å‹™é…¬è¼‰
            }

            db_client.add_task(download_task_id, json.dumps(download_payload), task_type='youtube_download')
            db_client.add_task(process_task_id, json.dumps(process_payload), task_type='gemini_process', depends_on=download_task_id)

            # JULES'S FIX: Return both task IDs so the frontend can track the full chain.
            tasks.append({
                "url": url,
                "task_id": download_task_id, # For display and initial tracking
                "final_task_id": process_task_id, # For listening to the final result
                "task_type": "youtube_process_chain"
            })

    return JSONResponse(content={"message": f"å·²ç‚º {len(tasks)} å€‹ URL å»ºç«‹è™•ç†ä»»å‹™ã€‚", "tasks": tasks})


def trigger_model_download(model_size: str, loop: asyncio.AbstractEventLoop):
    """
    åœ¨ä¸€å€‹å–®ç¨çš„åŸ·è¡Œç·’ä¸­åŸ·è¡Œæ¨¡å‹ä¸‹è¼‰ï¼Œä¸¦é€é WebSocket å›å ±çµæœã€‚
    é€™å€‹ç‰ˆæœ¬æœƒé€è¡Œè®€å– stdout ä¾†ç²å–å³æ™‚çš„ JSON é€²åº¦æ›´æ–°ã€‚
    """
    def _download_in_thread():
        log.info(f"ğŸ§µ [åŸ·è¡Œç·’] é–‹å§‹ä¸‹è¼‰æ¨¡å‹: {model_size}")
        try:
            tool_script_path = ROOT_DIR / "src" / "tools" / ("mock_transcriber.py" if IS_MOCK_MODE else "transcriber.py")
            cmd = [sys.executable, str(tool_script_path), "--command=download", f"--model_size={model_size}"]

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                bufsize=1 # Line-buffered
            )

            # é€è¡Œè®€å– stdout ä»¥ç²å–é€²åº¦æ›´æ–°
            if process.stdout:
                for line in iter(process.stdout.readline, ''):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        # å»ºç«‹ WebSocket è¨Šæ¯
                        message = {
                            "type": "DOWNLOAD_STATUS",
                            "payload": {
                                "model": model_size,
                                "status": "downloading",
                                **data  # é€™æœƒåŒ…å« 'type', 'percent', 'description' ç­‰
                            }
                        }
                        asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)
                    except json.JSONDecodeError:
                        log.warning(f"[åŸ·è¡Œç·’] ç„¡æ³•è§£æä¾†è‡ª transcriber çš„ä¸‹è¼‰é€²åº¦ JSON: {line}")

            process.wait() # ç­‰å¾…ç¨‹åºçµæŸ

            # æ ¹æ“šç¨‹åºçš„è¿”å›ç¢¼æ±ºå®šæœ€çµ‚ç‹€æ…‹
            if process.returncode == 0:
                log.info(f"âœ… [åŸ·è¡Œç·’] æ¨¡å‹ '{model_size}' ä¸‹è¼‰æˆåŠŸã€‚")
                message = {
                    "type": "DOWNLOAD_STATUS",
                    "payload": {"model": model_size, "status": "completed", "progress": 100}
                }
            else:
                stderr_output = process.stderr.read() if process.stderr else "N/A"
                log.error(f"âŒ [åŸ·è¡Œç·’] æ¨¡å‹ '{model_size}' ä¸‹è¼‰å¤±æ•—ã€‚ Stderr: {stderr_output}")
                message = {
                    "type": "DOWNLOAD_STATUS",
                    "payload": {"model": model_size, "status": "failed", "error": stderr_output}
                }

            asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)

        except Exception as e:
            log.error(f"âŒ [åŸ·è¡Œç·’] ä¸‹è¼‰åŸ·è¡Œç·’ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            message = {
                "type": "DOWNLOAD_STATUS",
                "payload": {"model": model_size, "status": "failed", "error": str(e)}
            }
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)

    # å»ºç«‹ä¸¦å•Ÿå‹•åŸ·è¡Œç·’
    thread = threading.Thread(target=_download_in_thread)
    thread.start()


def trigger_transcription(task_id: str, file_path: str, model_size: str, language: Optional[str], beam_size: int, loop: asyncio.AbstractEventLoop, original_filename: Optional[str] = None):
    """
    åœ¨ä¸€å€‹å–®ç¨çš„åŸ·è¡Œç·’ä¸­åŸ·è¡Œè½‰éŒ„ï¼Œä¸¦é€é WebSocket å³æ™‚ä¸²æµçµæœã€‚
    """
    def _transcribe_in_thread():
        display_name = original_filename or file_path
        log.info(f"ğŸ§µ [åŸ·è¡Œç·’] é–‹å§‹è™•ç†è½‰éŒ„ä»»å‹™: {task_id}ï¼Œæª”æ¡ˆ: {display_name}")

        # å•é¡ŒäºŒï¼šå°‡æ‰€æœ‰è¼¸å‡ºçµ±ä¸€åˆ° uploads ç›®éŒ„ä¸‹ï¼Œä»¥ä¾¿æä¾›éœæ…‹æª”æ¡ˆæœå‹™
        output_dir = UPLOADS_DIR / "transcripts"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file_path = output_dir / f"{task_id}.txt"

        try:
            force_mock = os.environ.get("FORCE_MOCK_TRANSCRIBER") == "true"
            tool_script_path = ROOT_DIR / "src" / "tools" / ("mock_transcriber.py" if IS_MOCK_MODE or force_mock else "transcriber.py")
            cmd = [
                sys.executable,
                str(tool_script_path),
                "--command=transcribe",
                f"--audio_file={file_path}",
                f"--output_file={output_file_path}", # ä½¿ç”¨æ–°çš„è·¯å¾‘
                f"--model_size={model_size}",
            ]
            if language:
                cmd.append(f"--language={language}")
            cmd.append(f"--beam_size={beam_size}")

            log.info(f"åŸ·è¡Œè½‰éŒ„æŒ‡ä»¤: {' '.join(map(str, cmd))}")

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                bufsize=1
            )

            start_message_filename = original_filename or Path(file_path).name
            start_message = {
                "type": "TRANSCRIPTION_STATUS",
                "payload": {"task_id": task_id, "status": "starting", "filename": start_message_filename}
            }
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(start_message), loop)

            if process.stdout:
                for line in iter(process.stdout.readline, ''):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        if data.get("type") == "segment":
                            message = {
                                "type": "TRANSCRIPTION_UPDATE",
                                "payload": {"task_id": task_id, **data}
                            }
                            asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)
                    except json.JSONDecodeError:
                        log.warning(f"[åŸ·è¡Œç·’] ç„¡æ³•è§£æä¾†è‡ª transcriber çš„ JSON è¡Œ: {line}")

            process.wait()

            if process.returncode == 0:
                log.info(f"âœ… [åŸ·è¡Œç·’] è½‰éŒ„ä»»å‹™ '{task_id}' æˆåŠŸå®Œæˆã€‚")
                final_transcript = output_file_path.read_text(encoding='utf-8').strip()

                # å•é¡ŒäºŒï¼šå°‡æª”æ¡ˆç³»çµ±è·¯å¾‘è½‰æ›ç‚ºå¯å­˜å–çš„ URL
                final_result_obj = {
                    "transcript": final_transcript,
                    "transcript_path": convert_to_media_url(str(output_file_path)),
                    "output_path": convert_to_media_url(str(output_file_path)) # å¢åŠ ä¸€å€‹é€šç”¨çš„ output_path
                }
                db_client.update_task_status(task_id, 'completed', json.dumps(final_result_obj))
                log.info(f"âœ… [åŸ·è¡Œç·’] å·²å°‡ä»»å‹™ {task_id} çš„ç‹€æ…‹å’Œçµæœæ›´æ–°è‡³è³‡æ–™åº«ã€‚")

                final_message = {
                    "type": "TRANSCRIPTION_STATUS",
                    "payload": {"task_id": task_id, "status": "completed", "result": final_result_obj}
                }
            else:
                stderr_output = process.stderr.read() if process.stderr else "N/A"
                log.error(f"âŒ [åŸ·è¡Œç·’] è½‰éŒ„ä»»å‹™ '{task_id}' å¤±æ•—ã€‚è¿”å›ç¢¼: {process.returncode}ã€‚Stderr: {stderr_output}")
                final_message = {
                    "type": "TRANSCRIPTION_STATUS",
                    "payload": {"task_id": task_id, "status": "failed", "error": stderr_output}
                }

            asyncio.run_coroutine_threadsafe(manager.broadcast_json(final_message), loop)

        except Exception as e:
            log.error(f"âŒ [åŸ·è¡Œç·’] è½‰éŒ„åŸ·è¡Œç·’ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            error_message = {
                "type": "TRANSCRIPTION_STATUS",
                "payload": {"task_id": task_id, "status": "failed", "error": str(e)}
            }
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(error_message), loop)

    thread = threading.Thread(target=_transcribe_in_thread)
    thread.start()


def trigger_youtube_processing(task_id: str, loop: asyncio.AbstractEventLoop):
    """åœ¨ä¸€å€‹å–®ç¨çš„åŸ·è¡Œç·’ä¸­åŸ·è¡Œ YouTube è™•ç†æµç¨‹ï¼ˆå·²æ›´æ–°ç‚ºå½ˆæ€§æ¨¡å¼ï¼‰ã€‚"""
    def _process_in_thread():
        log.info(f"ğŸ§µ [åŸ·è¡Œç·’] é–‹å§‹è™•ç† YouTube ä»»å‹™éˆï¼Œèµ·å§‹ ID: {task_id}")

        task_info = db_client.get_task_status(task_id)
        if not task_info:
            log.error(f"âŒ [åŸ·è¡Œç·’] æ‰¾ä¸åˆ°èµ·å§‹ä»»å‹™ {task_id}")
            return

        task_type = task_info.get('type')
        dependent_task_id = None

        try:
            payload = json.loads(task_info['payload'])
            url = payload['url']
            custom_filename = payload.get("custom_filename")
            download_type = payload.get("download_type", "audio")

            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": task_id, "status": "downloading", "message": f"æ­£åœ¨ä¸‹è¼‰ ({download_type}): {url}", "task_type": task_type}
            }), loop)

            # JULES DEBUG (2025-08-31): ç‚ºäº† E2E æ¸¬è©¦çš„ç©©å®šæ€§ï¼Œæ–°å¢ mock URL åˆ¤æ–·
            if url.startswith("mock://"):
                downloader_script_path = ROOT_DIR / "src" / "tools" / "mock_downloader_for_test.py"
            else:
                downloader_script_path = ROOT_DIR / "src" / "tools" / ("mock_youtube_downloader.py" if IS_MOCK_MODE else "youtube_downloader.py")

            cmd_dl = [sys.executable, str(downloader_script_path), "--url", url, "--output-dir", str(UPLOADS_DIR), "--download-type", download_type]
            if custom_filename:
                cmd_dl.extend(["--custom-filename", custom_filename])

            cookies_path = UPLOADS_DIR / "cookies.txt"
            if cookies_path.is_file():
                log.info(f"ç™¼ç¾ cookies.txtï¼Œå°‡å…¶ç”¨æ–¼ä¸‹è¼‰ã€‚")
                cmd_dl.extend(["--cookies-file", str(cookies_path)])

            proc_env = os.environ.copy()
            # JULES'S FIX (2025-08-30): é‡æ§‹ I/O è™•ç†ä»¥è§£æ±ºæ­»é–å•é¡Œ
            # èˆŠçš„å¯«æ³•æ˜¯é€è¡Œè®€å– stderrï¼Œä½†å¦‚æœ stdout çš„ç·©è¡å€è¢«å¡«æ»¿ï¼Œå­ç¨‹åºæœƒè¢«é˜»å¡ï¼Œ
            # è€Œçˆ¶ç¨‹åºå»åœ¨ç­‰å¾… stderrï¼Œå¾è€Œå°è‡´æ­»é–ã€‚
            #
            # æ–°çš„å¯«æ³•ä½¿ç”¨ communicate()ï¼Œå®ƒæœƒå®‰å…¨åœ°è®€å–å…©å€‹æµç›´åˆ°ç¨‹åºçµæŸï¼Œ
            # é›–ç„¶æœƒå¤±å»å³æ™‚çš„é€²åº¦å›å ±ï¼Œä½†èƒ½å®Œå…¨é¿å…æ­»é–ï¼Œç¢ºä¿ä»»å‹™èƒ½æ­£ç¢ºå®Œæˆã€‚
            # é€™æ˜¯æ ¹æ“š POC æˆåŠŸæ¡ˆä¾‹çš„æ¨¡å¼é€²è¡Œçš„é‡æ§‹ã€‚
            process_dl = subprocess.Popen(
                cmd_dl,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                env=proc_env
            )

            # communicate() æœƒè®€å–æ‰€æœ‰è¼¸å‡ºç›´åˆ°ç¨‹åºçµæŸï¼Œä¸¦è¿”å›çµæœã€‚
            # é€™èƒ½æœ‰æ•ˆé¿å…ç·©è¡å€å¡«æ»¿å°è‡´çš„æ­»é–ã€‚
            stdout_output, stderr_output = process_dl.communicate()

            # åœ¨ç¨‹åºçµæŸå¾Œï¼Œæª¢æŸ¥è¿”å›ç¢¼
            if process_dl.returncode != 0:
                # å°‡ stderr çš„å…§å®¹åŒ…å«åœ¨éŒ¯èª¤è¨Šæ¯ä¸­ï¼Œä»¥ä¾¿é™¤éŒ¯
                log.error(f"âŒ [åŸ·è¡Œç·’] youtube_downloader.py åŸ·è¡Œå¤±æ•—ã€‚Stderr: {stderr_output}")
                # æˆ‘å€‘å¾ stdout ä¸­è§£æ JSONï¼Œå› ç‚ºå³ä½¿å¤±æ•—ï¼Œè…³æœ¬ä¹Ÿæœƒè¼¸å‡ºä¸€å€‹éŒ¯èª¤ JSON
                # ä½†å¦‚æœ stdout æ˜¯ç©ºçš„ï¼Œå°±ä½¿ç”¨ stderr ä½œç‚ºéŒ¯èª¤è¨Šæ¯
                if stdout_output:
                    raise RuntimeError(stdout_output)
                else:
                    raise RuntimeError(f"youtube_downloader.py åŸ·è¡Œå¤±æ•—ï¼Œè¿”å›ç¢¼ {process_dl.returncode}ã€‚éŒ¯èª¤: {stderr_output}")

            # å¦‚æœæˆåŠŸï¼Œstdout æ‡‰è©²åŒ…å«æœ€çµ‚çš„ JSON çµæœ
            download_result = json.loads(stdout_output)
            media_file_path = download_result['output_path'] # This is an absolute path
            video_title = download_result.get('video_title', 'ç„¡æ¨™é¡Œå½±ç‰‡')
            log.info(f"âœ… [åŸ·è¡Œç·’] YouTube åª’é«”ä¸‹è¼‰å®Œæˆ: {media_file_path}")

            if task_type == 'youtube_download_only':
                # å•é¡ŒäºŒï¼šå°‡æª”æ¡ˆç³»çµ±è·¯å¾‘è½‰æ›ç‚ºå¯å­˜å–çš„ URL
                download_result['output_path'] = convert_to_media_url(download_result['output_path'])
                db_client.update_task_status(task_id, 'completed', json.dumps(download_result))
                log.info(f"âœ… [åŸ·è¡Œç·’] 'åƒ…ä¸‹è¼‰åª’é«”' ä»»å‹™ {task_id} å®Œæˆã€‚")
                asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                    "type": "YOUTUBE_STATUS",
                    "payload": {"task_id": task_id, "status": "completed", "result": download_result, "task_type": "download_only"}
                }), loop)
                return

            db_client.update_task_status(task_id, 'completed', json.dumps(download_result))
            dependent_task_id = db_client.find_dependent_task(task_id)
            if not dependent_task_id:
                raise ValueError(f"æ‰¾ä¸åˆ°ä¾è³´æ–¼ä¸‹è¼‰ä»»å‹™ {task_id} çš„ gemini_process ä»»å‹™")

            process_task_info = db_client.get_task_status(dependent_task_id)
            process_payload = json.loads(process_task_info['payload'])
            model = process_payload['model']
            tasks_to_run = process_payload.get('tasks', 'summary,transcript')
            output_format = process_payload.get('output_format', 'html')
            api_key = process_payload.get('api_key') # å¾ä»»å‹™é…¬è¼‰ä¸­è®€å–é‡‘é‘°

            log.info(f"åŸ·è¡Œ Gemini åˆ†æï¼Œä»»å‹™: '{tasks_to_run}', æ ¼å¼: '{output_format}'")
            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": dependent_task_id, "status": "processing", "message": f"ä½¿ç”¨ {model} é€²è¡Œ AI åˆ†æ...", "task_type": "gemini_process"}
            }), loop)

            processor_script_path = ROOT_DIR / "src" / "tools" / ("mock_gemini_processor.py" if IS_MOCK_MODE else "gemini_processor.py")
            # å•é¡ŒäºŒï¼šå°‡å ±å‘Šä¹Ÿè¼¸å‡ºåˆ° uploads ç›®éŒ„ä¸‹
            report_output_dir = UPLOADS_DIR / "reports"
            report_output_dir.mkdir(parents=True, exist_ok=True)

            cmd_process = [
                sys.executable, str(processor_script_path),
                "--command=process",
                "--audio-file", media_file_path,
                "--model", model,
                "--output-dir", str(report_output_dir),
                "--video-title", video_title,
                "--tasks", tasks_to_run,
                "--output-format", output_format
            ]

            proc_env = os.environ.copy()
            if api_key:
                proc_env["GOOGLE_API_KEY"] = api_key # å°‡é‡‘é‘°è¨­å®šåˆ°å­ç¨‹åºçš„ç’°å¢ƒè®Šæ•¸ä¸­

            log.info(f"ä»»å‹™ {dependent_task_id}: æ­£è¦å•Ÿå‹• gemini_processor.py å­ç¨‹åº...")
            process_gemini = subprocess.Popen(
                cmd_process, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8', env=proc_env
            )

            # JULES'S FIX (2025-09-01): Refactor I/O handling to prevent deadlocks.
            # Reading stderr line-by-line while stdout buffer might fill up is a classic deadlock scenario.
            # The safer approach is to use communicate() to get both streams after the process finishes.
            # This sacrifices real-time progress updates for stability and correctness, which is the right trade-off here.
            log.info(f"ä»»å‹™ {dependent_task_id}: æ­£åœ¨ç­‰å¾… gemini_processor.py å­ç¨‹åºå®Œæˆ...")
            stdout_output, stderr_output = process_gemini.communicate()
            log.info(f"ä»»å‹™ {dependent_task_id}: gemini_processor.py å­ç¨‹åºå·²çµæŸã€‚è¿”å›ç¢¼: {process_gemini.returncode}")

            if process_gemini.returncode != 0:
                # Log the full stderr for debugging purposes, then raise the error with stdout,
                # as the tool is designed to put the final error JSON in stdout.
                log.error(f"âŒ [åŸ·è¡Œç·’] gemini_processor.py åŸ·è¡Œå¤±æ•—ã€‚Stderr: {stderr_output}")
                if stdout_output:
                    raise RuntimeError(stdout_output)
                else:
                    raise RuntimeError(f"Gemini processor failed with exit code {process_gemini.returncode}. Stderr: {stderr_output}")

            process_result = json.loads(stdout_output)
            # å•é¡ŒäºŒï¼šå°‡çµæœä¸­çš„æ‰€æœ‰æª”æ¡ˆè·¯å¾‘è½‰æ›ç‚º URL
            for key in ["output_path", "html_report_path", "pdf_report_path"]:
                 if key in process_result and process_result[key]:
                    process_result[key] = convert_to_media_url(process_result[key])

            db_client.update_task_status(dependent_task_id, 'å·²å®Œæˆ', json.dumps(process_result))
            log.info(f"âœ… [åŸ·è¡Œç·’] Gemini AI è™•ç†å®Œæˆã€‚")

            # JULES'S FIX (2025-08-31): è£œä¸Šéºå¤±çš„ WebSocket å»£æ’­
            final_payload = {
                "task_id": dependent_task_id,
                "status": "completed",
                "task_type": "gemini_process",
                "result": process_result
            }
            update_message = {"type": "YOUTUBE_STATUS", "payload": final_payload}
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(update_message), loop)
            log.info(f"âœ… [åŸ·è¡Œç·’] å·²å»£æ’­ Gemini AI ä»»å‹™å®Œæˆè¨Šæ¯ã€‚")

        except Exception as e:
            log.error(f"âŒ [åŸ·è¡Œç·’] YouTube è™•ç†éˆä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            failed_task_id = dependent_task_id if dependent_task_id else task_id
            error_payload = {"error": str(e)}
            try:
                error_json = json.loads(str(e))
                if isinstance(error_json, dict):
                    error_payload["error"] = error_json.get("error", str(e))
                    if error_json.get("error_code") == "AUTH_REQUIRED":
                        error_payload["error_type"] = "AUTH_REQUIRED"
            except (json.JSONDecodeError, TypeError):
                pass
            db_client.update_task_status(failed_task_id, 'failed', json.dumps(error_payload))
            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": failed_task_id, "status": "failed", **error_payload}
            }), loop)

    thread = threading.Thread(target=_process_in_thread)
    thread.start()


@app.post("/api/debug/clear_tasks", status_code=200)
async def clear_all_tasks_endpoint():
    """
    [åƒ…ä¾›æ¸¬è©¦] æ¸…é™¤æ‰€æœ‰ä»»å‹™ï¼Œç”¨æ–¼é‡ç½®æ¸¬è©¦ç’°å¢ƒã€‚
    """
    log.warning("âš ï¸ [åƒ…ä¾›æ¸¬è©¦] æ”¶åˆ°è«‹æ±‚ï¼Œå°‡æ¸…é™¤æ‰€æœ‰ä»»å‹™...")
    try:
        success = db_client.clear_all_tasks()
        if success:
            return {"status": "success", "message": "æ‰€æœ‰ä»»å‹™å·²æˆåŠŸæ¸…é™¤ã€‚"}
        else:
            raise HTTPException(status_code=500, detail="åœ¨ä¼ºæœå™¨ç«¯æ¸…ç†ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")
    except Exception as e:
        log.error(f"âŒ æ¸…ç†ä»»å‹™çš„ API ç«¯é»ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/debug/latest_frontend_action_log")
async def get_latest_frontend_action_log():
    """
    [åƒ…ä¾›æ¸¬è©¦] ç²å–æœ€æ–°çš„å‰ç«¯æ“ä½œæ—¥èªŒã€‚
    ç”¨æ–¼ E2E æ¸¬è©¦ï¼Œä»¥é©—è­‰æ—¥èªŒæ˜¯å¦å·²æˆåŠŸå¯«å…¥è³‡æ–™åº«ã€‚
    """
    try:
        # æˆ‘å€‘åªé—œå¿ƒä¾†è‡ª 'frontend_action' logger çš„æ—¥èªŒ
        logs = db_client.get_system_logs(sources=['frontend_action'])
        if not logs:
            # å¦‚æœæ²’æœ‰æ—¥èªŒï¼Œè¿”å›ä¸€å€‹æ¸…æ™°çš„ç©ºå›æ‡‰ï¼Œè€Œä¸æ˜¯ 404
            return JSONResponse(content={"latest_log": None}, status_code=200)

        # get_system_logs æŒ‰æ™‚é–“æˆ³å‡åºæ’åºï¼Œæ‰€ä»¥æœ€å¾Œä¸€å€‹å°±æ˜¯æœ€æ–°çš„
        latest_log = logs[-1]
        return JSONResponse(content={"latest_log": latest_log})
    except Exception as e:
        log.error(f"âŒ æŸ¥è©¢æœ€æ–°å‰ç«¯æ—¥èªŒæ™‚å‡ºéŒ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æŸ¥è©¢æœ€æ–°å‰ç«¯æ—¥èªŒæ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤")


@app.websocket("/api/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            log.info(f"å¾ WebSocket æ”¶åˆ°è¨Šæ¯: {data}")

            try:
                message = json.loads(data)
                msg_type = message.get("type")
                payload = message.get("payload", {})

                if msg_type == "DOWNLOAD_MODEL":
                    model_size = payload.get("model")
                    if model_size:
                        log.info(f"æ”¶åˆ°ä¸‹è¼‰ '{model_size}' æ¨¡å‹çš„è«‹æ±‚ã€‚")
                        await manager.broadcast_json({
                            "type": "DOWNLOAD_STATUS",
                            "payload": {"model": model_size, "status": "starting", "progress": 0}
                        })
                        loop = asyncio.get_running_loop()
                        trigger_model_download(model_size, loop)
                    else:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ç¼ºå°‘æ¨¡å‹å¤§å°åƒæ•¸"})

                elif msg_type == "START_TRANSCRIPTION":
                    task_id = payload.get("task_id")
                    if not task_id:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ç¼ºå°‘ task_id åƒæ•¸"})
                        continue

                    task_info = db_client.get_task_status(task_id)
                    if not task_info:
                        await manager.broadcast_json({"type": "ERROR", "payload": f"æ‰¾ä¸åˆ°ä»»å‹™ {task_id}"})
                        continue

                    try:
                        task_payload = json.loads(task_info['payload'])
                        file_path = task_payload.get("input_file")
                        model_size = task_payload.get("model_size", "tiny")
                        language = task_payload.get("language")
                        beam_size = task_payload.get("beam_size", 5)
                        original_filename = task_payload.get("original_filename") # JULES'S FIX
                    except (json.JSONDecodeError, KeyError) as e:
                        await manager.broadcast_json({"type": "ERROR", "payload": f"è§£æä»»å‹™ {task_id} çš„ payload å¤±æ•—: {e}"})
                        continue

                    if not file_path:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ä»»å‹™ payload ä¸­ç¼ºå°‘æª”æ¡ˆè·¯å¾‘"})
                    else:
                        display_name = original_filename or file_path
                        log.info(f"æ”¶åˆ°é–‹å§‹è½‰éŒ„ '{display_name}' çš„è«‹æ±‚ (ä¾†è‡ªä»»å‹™ {task_id})ã€‚")
                        loop = asyncio.get_running_loop()
                        trigger_transcription(task_id, file_path, model_size, language, beam_size, loop, original_filename=original_filename)

                elif msg_type == "START_YOUTUBE_PROCESSING":
                    task_id = payload.get("task_id") # This is the download_task_id
                    if not task_id:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ç¼ºå°‘ task_id åƒæ•¸"})
                        continue

                    log.info(f"æ”¶åˆ°é–‹å§‹è™•ç† YouTube ä»»å‹™éˆçš„è«‹æ±‚ (èµ·å§‹ä»»å‹™ ID: {task_id})ã€‚")
                    loop = asyncio.get_running_loop()
                    trigger_youtube_processing(task_id, loop)

                else:
                    await manager.broadcast_json({
                        "type": "ECHO",
                        "payload": f"å·²æ”¶åˆ°æœªçŸ¥é¡å‹çš„è¨Šæ¯: {msg_type}"
                    })

            except json.JSONDecodeError:
                log.error("æ”¶åˆ°äº†é JSON æ ¼å¼çš„ WebSocket è¨Šæ¯ã€‚")
                await manager.broadcast_json({"type": "ERROR", "payload": "è¨Šæ¯å¿…é ˆæ˜¯ JSON æ ¼å¼"})

    except WebSocketDisconnect:
        manager.disconnect(websocket)
        log.info("WebSocket ç”¨æˆ¶ç«¯å·²é›¢ç·šã€‚")
    except Exception as e:
        log.error(f"WebSocket ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        # ç¢ºä¿åœ¨ç™¼ç”ŸéŒ¯èª¤æ™‚ä¹Ÿä¸­æ–·é€£ç·š
        if websocket in manager.active_connections:
            manager.disconnect(websocket)


@app.get("/api/health")
async def health_check():
    """æä¾›ä¸€å€‹ç°¡å–®çš„å¥åº·æª¢æŸ¥ç«¯é»ã€‚"""
    return {"status": "ok", "message": "API Server is running."}




class AppStatePayload(BaseModel):
    key: str
    value: str

@app.post("/api/app_state", status_code=200)
async def set_app_state_endpoint(payload: AppStatePayload):
    """
    è¨­å®šä¸€å€‹æ‡‰ç”¨ç¨‹å¼ç‹€æ…‹å€¼ã€‚
    """
    try:
        success = db_client.set_app_state(payload.key, payload.value)
        if success:
            # å»£æ’­ç‹€æ…‹è®Šæ›´
            await manager.broadcast_json({"type": "APP_STATE_UPDATE", "payload": {payload.key: payload.value}})
            return {"status": "success", "key": payload.key, "value": payload.value}
        else:
            raise HTTPException(status_code=500, detail="ç„¡æ³•åœ¨è³‡æ–™åº«ä¸­è¨­å®šæ‡‰ç”¨ç¨‹å¼ç‹€æ…‹ã€‚")
    except Exception as e:
        log.error(f"âŒ è¨­å®šæ‡‰ç”¨ç¨‹å¼ç‹€æ…‹æ™‚ API å‡ºéŒ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="è¨­å®šæ‡‰ç”¨ç¨‹å¼ç‹€æ…‹æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ã€‚")

@app.get("/api/app_state", response_class=JSONResponse)
async def get_all_app_states_endpoint():
    """
    ç²å–æ‰€æœ‰æ‡‰ç”¨ç¨‹å¼ç‹€æ…‹å€¼ã€‚
    """
    try:
        states = db_client.get_all_app_states()
        return JSONResponse(content=states)
    except Exception as e:
        log.error(f"âŒ ç²å–æ‰€æœ‰æ‡‰ç”¨ç¨‹å¼ç‹€æ…‹æ™‚ API å‡ºéŒ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ç²å–æ‰€æœ‰æ‡‰ç”¨ç¨‹å¼ç‹€æ…‹æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ã€‚")


@app.post("/api/internal/notify_task_update", status_code=200)
async def notify_task_update(payload: Dict):
    """
    ä¸€å€‹å…§éƒ¨ç«¯é»ï¼Œä¾› Worker ç¨‹åºåœ¨ä»»å‹™å®Œæˆæ™‚å‘¼å«ï¼Œ
    ä»¥ä¾¿é€é WebSocket å°‡æ›´æ–°å»£æ’­çµ¦å‰ç«¯ã€‚
    """
    task_id = payload.get("task_id")
    status = payload.get("status")
    result = payload.get("result")
    log.info(f"ğŸ”” æ”¶åˆ°ä¾†è‡ª Worker çš„ä»»å‹™æ›´æ–°é€šçŸ¥: Task {task_id} -> {status}")

    # JULES'S FIX: æŸ¥è©¢ä»»å‹™é¡å‹ä»¥ç™¼é€æ­£ç¢ºçš„ WebSocket è¨Šæ¯
    task_info = db_client.get_task_status(task_id)
    task_type = task_info.get("type", "transcribe") if task_info else "transcribe"

    message_type = "TRANSCRIPTION_STATUS"
    if "youtube" in task_type or "gemini" in task_type:
        message_type = "YOUTUBE_STATUS"

    log.info(f"æ ¹æ“šä»»å‹™é¡å‹ '{task_type}'ï¼Œå°‡ä½¿ç”¨ WebSocket è¨Šæ¯é¡å‹: '{message_type}'")

    # ç¢ºä¿ result æ˜¯å­—å…¸æ ¼å¼
    if isinstance(result, str):
        try:
            result = json.loads(result)
        except json.JSONDecodeError:
            log.warning(f"ä¾†è‡ª worker çš„ä»»å‹™ {task_id} çµæœä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚")

    message = {
        "type": message_type,
        "payload": {
            "task_id": task_id,
            "status": status,
            "result": result,
            "task_type": task_type  # å°‡ task_type ä¹Ÿå‚³çµ¦å‰ç«¯
        }
    }
    await manager.broadcast_json(message)
    return {"status": "notification_sent"}


# --- ä¸»ç¨‹å¼å•Ÿå‹• ---
if __name__ == "__main__":
    import uvicorn
    import argparse

    parser = argparse.ArgumentParser(description="é³³å‡°éŸ³è¨Šè½‰éŒ„å„€ API ä¼ºæœå™¨")
    parser.add_argument(
        "--port",
        type=int,
        default=8001,
        help="ä¼ºæœå™¨ç›£è½çš„åŸ è™Ÿ"
    )
    args, _ = parser.parse_known_args()

    # JULES: ç§»é™¤æ­¤è™•çš„è³‡æ–™åº«åˆå§‹åŒ–å‘¼å«ã€‚
    # çˆ¶ç¨‹åº src/core/orchestrator.py å°‡æœƒè² è²¬æ­¤äº‹ï¼Œä»¥é¿å…ç«¶çˆ­æ¢ä»¶ã€‚

    # JULES'S FIX: The database logging is now set up via the app's lifespan event.
    # setup_database_logging() is no longer needed here.

    log.info("ğŸš€ å•Ÿå‹• API ä¼ºæœå™¨ (v3)...")
    log.info(f"è«‹åœ¨ç€è¦½å™¨ä¸­é–‹å•Ÿ http://127.0.0.1:{args.port}")
    uvicorn.run(app, host="0.0.0.0", port=args.port)
